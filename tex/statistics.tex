\documentclass[../main/main.tex]{subfiles}
\begin{document}

\chapter{Statistics}\label{statistics}
\section{Bayes' Theorem}\label{bayes-theorem}

I can never seem to remember Bayes' Theorem directly, as they write it
out in textbooks. It makes so much more sense to me to think about it
from the relationships between conditional and joint
probabilites/distributions, and one of the common tricks to make Bayes'
Theorem useful in practice also comes to me far more easily when
explicitly thinking about events as being sampled from a \emph{sample
space} of possibilities/outcomes.

Consider two possible events A and B. Let's keep in mind that A is just
one possible outcome out of a set of possibilities, as is B; we'll say
\(\alpha\) is the set of possibilities from which \(A\) was drawn and
\(\beta\) is the set of possibilities from which \(B\) was drawn, i.e.
\(A \sim \alpha\) and \(B \sim \beta\) (this will be good to remember
later). Now:

\[ \text{Pr[A and B both occur]} := P(A,B) \]

Assuming individual events happen separately, there are two ways for
both A and B to occur: 
\begin{enumerate}
  \tightlist
    \item
      A happens first, then B happens. 
    \item 
      B happens first, then A happens.
\end{enumerate}
(``Duh'', I know.)
\\
Keeping in mind that the first event might affect the probability of the
second event occurring (i.e. remembering that conditional probabilities
exist), we can write:

\[ P(A,B) = P(A) \times P(B \mid A) = P(B) \times P(A \mid B) \]

And then it's simple to write out Bayes' Theorem as it's often written
(we'll write it perhaps a bit more evocatively):

\[ P(B) \times P(A \mid B) = P(A) \times P(B \mid A) \]

\[\begin{split} P(A \mid B) &= \frac{P(A) \times P(B \mid A)} {P(B)} \\
                        &= P(A) \times \frac {P(B \mid A)} {P(B)} \end{split}\]

Using the Bayesian interpretation: At first we thought the probability
that \(A\) occurs is \(P(A)\). After we saw that \(B\) happened, we
re-evaluate the probability that \(A\) occurs with a scaling factor \(
\frac {P(B \mid A)} {P(B)} \), which answers the following question: considering I've
seen \(B\) occur, how much \emph{more} likely did I observe \(B\) due to
\(A\) also being the case (the numerator \(P(B \mid A)\)), versus my having observed
\(B\) just because of how common/rare it is (the denominator, \(P(B)\))? To see
why this scaling factor makes sense, let's consider some edge cases:

\begin{itemize}
  \tightlist
  \item 
    \emph{A and B are uncorrelated}: Then observing B is irrelevant when it comes
    to predicting A. So our scaling factor should be 1. And in fact, the lack of 
    correlation implies \(P(B \mid A) = P(B) \implies \frac {P(B \mid A)} {P(B)} = 1\).
  \item
    \emph{A precludes B}: Then via contrapositivity, if we saw B, A must not be the case.
    So our scaling factor should be 0. And since A precludes B,
    \(P(B \mid A) = 0 \implies \frac {P(B \mid A)} {P(B)} = 0\).
  \item
    \emph{B implies A (and no other outcome from \(\alpha\))}:
    Then the total probability
    must become 1, and the scaling factor must come out to be \(\frac {1} {P(A)}\).
    We'll come back to this in a bit.
\end{itemize}

Also worth knowing the fancy terminology: 
\begin{enumerate}
  \tightlist
  \item
    the \textbf{\textit{a priori} probability} or just the \textbf{prior} 
    \index{prior (Bayesian statistics)}
    is what 
    we thought would be the
    probability that a random variable takes on a certain value before we
    observed anything. So the \emph{a priori} probability (or just prior)
    for the event \(A\) would be \(P(A)\). If we consider \(A\) to be a
    random variable instead of an event, we're guessing the distribution of
    \(A\) and so \(P(A)\) would be an \textbf{\textit{a priori} distribution}
    (or again, just the prior). 
  \item
    the \textbf{\textit{a posteriori}
    probability} or just the \textbf{posterior}
    \index{posterior (Bayesian statistics)}
    is what we think the
    probability that a random variable takes on a certain value is after
    observing something. In this case, the \emph{a posteriori} probability
    (or just posterior) of the event \(A\) after observing \(B\) is
    \(P(A \mid B)\). If we consider \(A\) to be a random variable instead of
    an event, we're guessing the distribution of \(A\) after observing a
    random variable/event B and so \(P(A \mid B)\) would be an
    \textbf{\textit{a posteriori} distribution}, (or again, just the
    posterior).
\end{enumerate}

\subsection*{Substitution to the rescue.}\label{subsitution-to-the-rescue}
Now, in cases of inference via supervised learning, 
we have some data on the probability of one of the observable variables
\textemdash{} let's say we observe variables \(A \sim \alpha\) \textemdash{}
and the labels/targets for the observed variables (let's say \(B ~ \beta\)).
Then we can approximate \(P(B \mid A\)) and \(P(A)\) via empirical counts \textemdash{}
and would want to fit a continuous function, e.g. a Gaussian, to \(P(A)\) to
handle out-of-sample feature combinations.
So we've already guessed some prior \(P(A)\), and we're trying to improve it by
calculating the posterior \( P(A \mid B) \). But what if we don't know
\( P(B) \)? Do we need to also guess a function for \( P(B) \)? Well, that would
involve another outside assumption which may or may not be true, and usually we want
to make as few assumptions as possible and
``let the data speak for itself''.
So then is our guessing and data collection all for naught!?
Thankfully, not so! The answer lies right under our noses -\/- or in
this case, in our previous calculations.

Consider \(P(A,B)\) again. What would we get if we added \(P(A,B)\) over
all possible values of A? (Remember we said that \(A \sim \alpha\), so A
could have been some other event within the set \(\alpha\).) That's
basically just saying that we don't care what value \(A\) is, so we end
up with \(P(B)\)!\footnote{
  `!' used to denote excitement, not factorialization.
  }
And conveniently, we'd already have a way to estimate these values:

\[\begin{split} P(B) &= \sum_{A \in \alpha} P(A,B) \\
                     &= \sum_{A \in \alpha} P(A) \times P(B \mid A)          \end{split}\]

Our summand is the same as the values we've estimated either by guessing
(\(P(A)\)) or from our data (\(P(B \mid A)\))! With that, we can rewrite
our earlier equation as

\[ P(A \mid B) = 
  \frac{P(A) \times P(B \mid A)} {\sum_{A' \in \alpha} P(A') \times P(B \mid A')} \]

With that, we can crunch the numbers and perform Bayesian inference like
a champ or have a machine do it for us like a prudent delegator.
\\
\\
Returning to our discussion of our scaling factor, we needed to show that if
we're trying to predict \(P(A \mid B)\) via
\[P(A \mid B) = P(A) \times \frac {P(B \mid A)} {P(B)} \]
and B implies A (and nothing else from \(\alpha\), then the right-hand side should equal 1.

Expanding P(B), we get
\[ P(A \mid B) = 
  \frac{P(A) \times P(B \mid A)} {\sum_{A' \in \alpha} P(A') \times P(B \mid A')} \]

Via our implication, we have that \(P(B \mid A') = 0 \forall A' \neq A\).
So the denominator simplifies to just \(P(A) \times P(B \mid A)\), which is exactly the
numerator! So then \(\left(B \implies A\right) \implies P(A \mid B) = 1\) (which implies
\( \frac {P(B \mid A)} {P(B)} = \frac {1} {P(A)} \)), as expected.
% Though
% for our everyday activities, we often don't have the luxury of having
% someone/something checking our heuristics. So it's always worth trying
% to keep in mind that oftentimes, many different factors that you may not
% know or take into consideration can culminate in observations that
% surprise you -\/- there's a good reason you aren't told to constantly
% get yourself tested for a medical condition if you don't believe to be
% at risk!
\\
\\
- DK (4/24/18)

\newpage

\section{Testing multiple hypotheses.}

\subsection{Comical correlations.}

There are some wonderfully comical examples of making questionable statistical
interpretations.
One example is literally a comic: the webcomic \textit{xkcd} published an example where
\href{https://xkcd.com/882/}{it is ``shown'' that green jelly beans are linked to acne}.
In another (perhaps more striking) context,
data mining is prone to discovering all sorts of questionable 
``correlations'', as shown in 
\href{http://www.tylervigen.com/spurious-correlations}{Tyler Vigen's collection
of such conclusions}, where it is ``shown'' that (among other facts) 
that US per-capita margarine consumption
is linked to the divorce rate in Maine.\par

But what's the problem? All of xkcd's statistical experiments were run
at the ``industry-standard'' confidence level of \(\alpha = 0.05\),
and confidence intervals on the data shown
on Vigen's website would have lower bounds well above \(r = 0\) 
even at a 99.9\% confidence level.
So the conclusions made in both are totally valid, right?\par

...But of course they aren't \textemdash{} at least, not as it stands.
The fact is both of the scenarios above are testing (far) more than
one hypothesis: 21 tests total in the xkcd case, and who knows how many millions
of tests in the data mining case.\par




\subsection{The problem: confounding exploratory vs confirmatory analyses.}

The thing is, there's nothing inherently wrong with performing
all these statistical tests as had been done above.
Data mining in particular is very good as a tool for
\emph{exploratory data analysis}, which is meant to
help a researcher generate hypotheses that are
potentially worth testing. It's a \emph{start}, after which
proper statistical protocol should be established: 
what population is under study, what effect is being looked at,
how the sample was gathered (with rationale),
which statistical test will be used and at what confidence
level (with rationale), etc etc. \par

The more concerning part is taking the initial point of significance
as evidence of an actual correlation.
Remember that a significance level \(\alpha\) corresponds to the
frequency at which we would commit a 
\emph{Type I error}, 
wherein we reject the null hypothesis due to effects caused by random chance alone
(and not due to any actual meaningful connections between
the variables in question).
So when you churn through \(k\) independent
hypothesis tests with corresponding significance levels 
\(\alpha_i\), the likelihood that you reject at least
one null hypothesis due to random chance is
\[FDR = \text{Pr[at least one Type I error]} = 
  1 - \text{Pr[no Type I errors]} = 1 - \prod_{i=1}^{k} (1 - \alpha_i) \]

And so we can't really go back and say that any of the
rejected hypotheses have significance level \(\alpha_i\)
\textemdash{} you can only be roughly \((1 - FDR)\)-confident
that the hypothesis was rejected due to something other
than random chance. \par

Alright, then how \emph{could} you establish a test with
your desired significance level of \(\alpha_i\)?
With an entirely new iteration of the statistical test, 
done all on its lonesome, 
with a brand new batch of data\footnote{
  This is done so that we aren't
  ``poisoning'' our sample with data we already would
  lead to rejecting the null hypothesis. If we \emph{did} 
  reuse the data,
  we'd be introducing some heavy bias, which is no good.
}. Only then would you have the false positive rate actually 
matching the intended value of \(\alpha_i\).\footnote
{
  This method of validation 
  (as well as the likely observation that the first
  null hypothesis rejection was a Type I error) 
  is implied in the alt-text of the xkcd comic 
  \textemdash{}
  thankfully the allure of Minecraft did not prevent
  the scientists from doing their statistician's duty.
}




\subsection{The Bonferroni correction: fewer false positives (and more false negatives).}

OK, but say we \emph{don't} have the ability to just
generate a fresh batch of samples to run clean statistical
tests
on the more ``promising'' hypotheses from an initial
exploratory analysis.
But we \emph{still} want to run our huge battery of tests
\emph{and} we want any statistically significant ``discoveries''
we may find to have a false positive rate of at most \(\alpha\).
Well, we can do that! But the notion that 
``there is no such thing as a free lunch'' applies here as well.
This applies in at least two ways.
One of these sacrifices is inescapable as it is intrinsic to the relationship
between false positives and false negatives \textemdash{}
since we're finally going to pull down our false positive rate
to our desired level, we necessarily have an increase in
our \emph{false negative rate}.
The other is that by applying the correction we're about
to discuss, we may end up being \emph{too} conservative
and have an effective false positive rate \(FPR < \alpha\).
This one is more manageable.\par
% That is to say, 
% by demanding that our statistic \(\hat{\mu} = \mu_0 + \Delta\) 
% deviate largely from the
% null hypothesis's parameter \(\mu_0\), 
% we're far less likely to notice if the true parameter is 
But enough with all the buildup! What's the correction?\par

Say you are testing \(k\) hypotheses and would like a total
false positive rate of \(\alpha\). Then you may choose
a set of individual significance levels \(\alpha_i\) such that

\[\sum_{i = 1}^{k} \alpha_i = \alpha \]

A particularly simple realization of the above is achieved
by simply ``splitting the difference'' equally among the
\(k\) hypotheses:

\[ \alpha_i \setequalto \frac{\alpha}{k} \]

Now, I know what you're saying.
\textit{``Really? All that buildup just to divide 
a number by another number?''}
And yes, really that's the case. But just because it's
simple doesn't mean it's unimportant! 
After all, it satisfies the desired criterion of having a
false positive rate of at most \(\alpha\). This is
verifiable by \emph{Boole's inequality}:
\begin{equation}
%   \begin{split}
%    \text{Pr[at least one Type I error]} &\leq 
%    \sum_{i=1}^{k}\text{Pr[Type I error on hypothesis }i\text{]} 
%   %  - \sum_{j=1}^{k-1}\left(\binom{k-1}{j})
%   \\
%   &= \sum_{i=1}^{k}\alpha_i \\
%   &= \alpha
% \end{split}
   \text{Pr[at least one Type I error]} \leq 
   \sum_{i=1}^{k}\text{Pr[Type I error on hypothesis }i\text{]} 
  = \sum_{i=1}^{k}\alpha_i
  = \alpha
\label{equation:bonferri-proof-loose}
\end{equation}

A graphical intuition for Boole's inequality can be gleaned by
picturing the Venn diagram of events corresponding to
the errors and considering how the union of the ``areas''
of each event would relate to their sum:
\[P(A \cup B) \leq P(A) + P(B) \]

This is extensible to an arbitrary number of events
\(A, B, C, \cdots\).
The 
\href{https://en.wikipedia.org/wiki/Inclusionâ€“exclusion_principle}{inclusion-exclusion principle}
shows an exact method of calculating the relationship,
but the main point is that the right-hand side of
the first line in
\ref{equation:bonferri-proof-loose} would have a nonpositive
term were it an equality, so the inequality is valid.\footnote
{
  The inclusion-exclusion principle applies because 
  the probability of at least one Type I error is a
  (normalized) union of the events which cause Type I errors.
}
\subsection{Thinking about M\&M's some more.}

Let's work along the xkcd example.
The last twenty hypotheses were essentially the same
with no reason
to believe any of them would be significant
and (presumably) no difference in the statistical methods
used to reach the p-value.
We match their conditions and have \(\alpha_i \setequalto 0.05\)
for each of our \(k=20\) tests.
Then (assuming independence between tests) the likelihood of 
rejecting a null hypothesis due to random chance alone within our \(k\) tests
is not \(\alpha_i = 0.05\) 
(as the cover of the sensationalist magazine in the webcomic implies)
but rather

\[
  FWER = \text{Pr[at least one Type I error]} = 
  1 - \text{Pr[no Type I errors]} = 1 - \prod_{i=1}^{k} (1 - \alpha_i) \approx 0.64 
\]

which is a far less impressive threshold of significance.
Here, \(FWER\) is the \emph{familywise error rate}, where
our ``family'' consists of the twenty nearly identical
hypothesis tests.\par

Now let's assume that M\&M's have become
a treasured commodity\footnote
{
  This may not differ from reality, depending on who you ask.
} and are difficult to come across,
or for whatever other reason we wouldn't be able to
perform a follow-up experiment to validate any
exploratory analysis above. 
So, before we perform our family of tests,
we apply a Bonferroni correction via
``splitting the difference'', i.e.,
\(\alpha_i' \setequalto 0.05/20 = 0.0025\) for all \(i\).
The amount of evidence required to reject any individual
hypothesis increases from a deviation of 
\(\Delta \approx 1.6\hat{\sigma}\) to 
\(\Delta \approx 2.8\hat{\sigma}\)
\textemdash{} a pretty big deal for z-tests and t-tests
of just about any size! Now we have a familywise error rate of

\[FWER = \text{Pr[at least one Type I error]} = 
  1 - \text{Pr[no Type I errors]} = 1 - \prod_{i=1}^{k} (1 - \alpha_i') \approx 0.0488 \]

Which is definitely less than our maximum tolerated
Type I error rate of \(\alpha = 0.05\).

\subsection{Slightly less conservative: the Sidak correction.}

We saw at the end of our example that our familywise error rate
was smaller than our target of 0.05 and so is more conservative
than we originally wanted. Indeed, with the Bonferroni
correction it is always the case that \(FWER < \alpha\).
The simplest, ``split the difference equally'' method in fact
gives us the
most conservative such \(FWER\).\footnote
{
  This comes from the fact that given
  the constraints \(\sum_i^{k} a_i = a, a_i > 0\), 
  \(\prod_i a_i\) is maximized when \(a_i \setequalto a/k\).
  For the problem in question, \(a = k-\alpha\)
  for a desired significance level \(\alpha\) and so
  \(a_i \setequalto 1 - \alpha/n\) maximizes their product
  \(\prod_i a_i\).
  But then we're subtracting this product from unity,
  so the choice of \(a_i\) in fact minimizes
  \(1 - \prod_i a_i = FWER\).
}
\footnote
{
  Fun fact: from the form of the expression for \(FWER\),
  we can see that using the ``split the difference equally''
  model, as we test arbitrarily many hypotheses,
  \[
    \lim_{k \to \infty} FWER = 
    \lim_{k \to \infty} \left(1 - \prod_{i=1}^{k} (1 - \alpha/k)\right)
    = \lim_{k \to \infty} \left(1 - (1 - \alpha/k)^k\right)
    = 1 - \lim_{k \to \infty} \left(1 - \frac{\alpha}{k}\right)^{k}
    = 1 - e^{-\alpha}
  \]
  \sout{That silly \(e\), getting itself into all sorts of places!}
}
So, how could we have our total Type I error rate
be \emph{exactly} \(\alpha\) instead of \emph{at most}
\(\alpha\)?
Well, we've kind of been playing with the relevant
equation for a while now. We want \(FWER = \alpha\), so
(once again assuming \(k\) hypotheses with
significance levels \(\alpha_i\)):

\[
  FWER = \text{Pr[at least one Type I error]} = 
  1 - \text{Pr[no Type I errors]} = 1 - \prod_{i=1}^{k} (1 - \alpha_i) \setequalto \alpha
\]

If we decide we'll use the same significance level
for all the individual hypotheses, 
say \(\alpha_i \setequalto s\), then we can simplify
and solve for \(s\) in terms of \(\alpha\):

\[\begin{split}
  \alpha &= 1 - \prod_{i=1}^{k} (1 - s) \\
   &= 1 - (1 - s)^k \\
   &\cdots \\
   s &= 1 - \left(1 - \alpha\right)^{1/k}
\end{split}
\]

This method of accounting for familywise error rate,
setting \(\alpha_i \setequalto s\),
is known as the
\textbf{Sidak correction}\index{Sidak correction}.\footnote{
  The font I've chosen apparently cannot handle some
  of the characters needed in Sidak's actual name.
  \href{https://en.wikipedia.org/wiki/\%C5\%A0id\%C3\%A1k_correction}{The Wikipedia article}
  on the correction bears the proper rendering.
}

\subsection{Conclusion.}
There are many other clever methods of properly
limiting the error rate of a family of hypothesis tests,
such as the \href{https://en.wikipedia.org/wiki/Holm%E2%80%93Bonferroni_method}{Holm-Bonferroni method}, with more fine-tuned
methods for controlling the \href{https://en.wikipedia.org/wiki/False_discovery_rate}{false discovery rate}
available.
It's always worth remembering whether it makes
sense to decrease the false positive rate at the cost
of increasing the false negative rate \textemdash{}
which mistake would be worse?
There's also the importance of the quality of the
sample we're studying \textemdash{} if we select a
specific subset of a population for our tests,
this \emph{selection bias} will limit our ability
to draw conclusions about populations outside
that subset.

This is a dance
with which clinical trial designers
ought to be really familiar, as 
when a trial is designed to test \emph{efficacy} 
(``Under ideal circumstances, does this intervention
improve the outcomes of that specific subset of
the population which we targeted in the first place?''),
you're necessarily sacrificing the trial's ability
to speak to its generalizable \emph{effectiveness}
(a sample of volunteers is the quintessential example
of the self-selection bias, and most patients in the
target population would not undergo the treatment 
at quite the same level of fastidiousness 
demanded by a clinical trial).

And of course, not much can save questionable
testing practices (e.g., forming a hypothesis based
on exploratory analysis of a dataset and then
testing the hypothesis on the very same set of
\textemdash{} 
no, I don't even want to finish the sentence!)

Like any tool, one must be cognizant of when
and how to properly 
use false-positive controlling procedures
such as the Bonferroni correction.

\subsection*{References and related links.}

The first mention of the Bonferroni method I heard
was from it being mentioned in passing in
one of the first episodes of the
% \href{https://dataskeptic.com/podcast/}{\textit{Data Skeptic} podcast}
\textit{Data Skeptic} podcast. (I would link
to the website, but it seems they've had a hiccup
with renewing their SSL certificates so it's broken
at the moment. How I understand their pain!)\par

\href{https://www.ncbi.nlm.nih.gov/pubmed/24697967}
{This 2014 paper by Armstrong} discussed
the use
of the Bonferroni correction in various ophthalmological 
publications and gave some good relevant reminders
about when it should be used. 
It helped me remember the ``Statistics 101''
concept that \[\text{false positive rate} \propto \frac{1}{\text{false negative rate}}\] 

A thorough discussion about
performing meta-regressions (trying to \emph{quantify}
the relationship between variables as opposed to
simply state \emph{whether} they seem related) is
\href{https://www.ncbi.nlm.nih.gov/pubmed/12111920}{this 2002 paper by Thompson} (certainly still relevant).
Just one of the important points brought up in the paper is
\href{https://en.wikipedia.org/wiki/Ecological_fallacy}{the ecological fallacy},
a formal fallacy where we expect a single individual
to be similar (in the aspect under consideration) 
to that of the population to which they belong.
Probably the most striking and 
commonly discussed form of the ecological fallacy is
\href{https://en.wikipedia.org/wiki/Simpson%27s_paradox}{Simpson's paradox (or the reversal paradox)}.
\par

\href{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1124898/}
{This fun 2002 paper} laments how spurious correlations
from observational studies that could have been
better controlled
find themselves on the cover of magazines
as the ``health scare of the week''.
\sout{Oh, how so much has changed \(\sim\).}
It also talks about why the spurious correlations occur
and some methods of controlling for them
(e.g., repeating tests on different to mitigate
the almost certain existence of confounding variables
\textemdash{} if the association exists across
many different populations, it's less likely to
be due to a hidden confounding variable).
\\
\\
- DK, 7/3/18

\end{document}