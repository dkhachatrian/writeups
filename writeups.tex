% Let's remember, if we're looking for a particular symbol, this link can help:
% http://detexify.kirelabs.org/classify.html

\documentclass[letterpaper,12pt]{report}
    \usepackage[margin=1in,letterpaper]{geometry}%%his shaves off default margins (which are too big)
    \usepackage{amsmath} %This lets us do more advanced math stuff -- in particular create unnumbered equations
    \usepackage{graphicx} %This lets us add figures
    \usepackage{float} %This lets us put figures smack in the middle of text, if we need to.
    \usepackage{grffile} %To prevent filenames from showing
    \usepackage{siunitx} %To put units properly, allow \SI[parse-numbers=false (if necessary)]{value}{unit}
    \usepackage{booktabs} %for fancier tables
    \usepackage{amssymb} % gives some fun symbols to play with, e.g. $\blacksquare$
    \usepackage[bottom]{footmisc} % have footnotes at bottom of page
    \usepackage{enumerate}
    \usepackage[normalem]{ulem} % allows for strikeout with \sout{}
    \usepackage{xcolor}% http://ctan.org/pkg/xcolor
    \usepackage{hyperref}% allow for links via \url{} and \href{}: http://ctan.org/pkg/hyperref
    \hypersetup{% because the red outline boxes are very bleh
      colorlinks=true,
      % allcolors=purple
      urlcolor = blue,
      linkcolor = purple
    }
    \usepackage{titlesec}
    \usepackage{enumitem} % be able to reference specific items in a list
    % \usepackage{bookmark} % maybe fixes silly refuse-to-compile-for-no-reason issues?
    %         % i.e. "File '*.out' has changed (rerunfilecheck) Rerun to get outlines right (rerunfilecheck) or use package 'bookmark'."

    \titleformat{\chapter}{\bfseries\Huge}{\thechapter.}{1.5ex}{}
    \titlespacing{\chapter}{0pt}{10pt}{30pt}









    \usepackage{bbm} % allow for fancy-case of numbers





    \usepackage{makeidx} % allows for indexing
    \makeindex



% from pandoc conversion (from markdown to LaTeX)
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% from jupyter notebook conversion (I can agree with the font choice)
  \usepackage[T1]{fontenc}
  % Nicer default font (+ math font) than Computer Modern for most use cases
  \usepackage{mathpazo}


\begin{document}

% commands




% Document parameters
\title{A quick(ish) reference for some concepts whose
 intuitions/simple methods of understanding sometimes escape me.}
\author{David Khachatrian}
\date{Ongoing.}

\maketitle

\newpage


\tableofcontents

\newpage


\chapter{Information Theory}\label{information-theory}

\section{Entropy and Cross-Entropy}\label{entropy-and-cross-entropy}

There are a number of ways to think about/get to the Shannon entropy
(\(H\)). One simple way is to say what we want
it to mean qualitatively and impose some desired characteristics to
determine its mathematical formulation. In this case, it may make more
sense to start with cross-entropy first.

Say you're sending me messages from an alphabet of symbols \(A\). Some
are more likely than others, and we'll call the probability distribution
associated with the actual generator of symbols \(p\). I'm over here on
the other side thinking like I'm a pair of smartypants that
has figured out how likely you are to send each symbol to me. We'll call
my expected distribution (which may or may not be the correct
distribution) \(q\).

Now we want to measure how surprised I am by any given message you send
me -\/- let's call it \(\tau\).\footnote{
    Rhymes with ``wow''. 
    Alas, not standard notation for a measure of surprise.
    }
We would imagine the following properties would be useful for \(\tau\) to have:
\begin{enumerate}
  % \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    The more surprised I am, the larger
    \(\tau\) gets (otherwise, it wouldn't exactly be doing a good job
    measuring my surprise) 
  \item
    If symbols are independently generated, we can
    construct the total surprise of my message by adding the total surprise
    of each symbol in the message, i.e.,
    \(\tau_{M} = \sum_{i=1}^{m} \tau_{M[i]}\) where \(M \in A^m\) is a
    string of symbols, \(m\) is the cardinality of \(M\), and \(M[i]\)
    denotes the \(i\)'th element of \(M\).
\end{enumerate}


Using these two criteria, a reasonable mapping is

\[\tau_{a \sim q} = \log\left(\frac{1}{q(a)}\right), a \in A\]

where \(q(a)\) is the probability I think you'll send the symbol \(a\).

I mean, that's great and all, but that works for individual instances of
strings or symbols. How much should I \emph{expect} to be surprised by
any given symbol? Well, that'd depend on how often you \emph{actually}
send me a symbol, alongside how often I expect to receive that
particular symbol.

Hmm, this smells of expectation! And indeed, that's all we do -\/- take
the expectation over \(A\) (using the \emph{actual} probability of each
symbol occurring, i.e., using \(p\)) of my surprise per symbol:

\[E_{a \sim p}[\tau_{a \sim q}] = \sum_{a \in A} p_a \log\left(\frac{1}{q(a)}\right) = H(p,q)\]

We denote this metric \(H(p,q)\) (where \(p\) and \(q\) are the actual
and presumed distributions, respectively) and call it the
\textbf{cross-entropy} \index{cross-entropy}
(because that sounds pretty cyberpunk to me. I
like to think they throw in ``cross'' because it measures the surprise
caused by ``crossing'' the distributions \(p\) and \(q\) together.)

Now, we'd imagine that I'd be the least surprised if my presumed
distribution of symbols were in fact the actual distribution, i.e. when
\(p = q\). In fact, this is true!
\[H(p,p) = H(p) = \sum_{a \in A} p_a \log\left(\frac{1}{p_a}\right)\]

is called the \textbf{entropy} (or \textbf{Shannon entropy})
\index{entropy (information theory)!for distributions}
of the
probability distribution and written \(H\).\footnote
{
  You know, it'd arguably make more sense for the 
  \emph{\textbf{S}hannon} entropy to be denoted \(S\),
  which would also be a happy notational coincidence 
  with the symbol used for entropy in most other fields. 
  Instead, we have a notational collision with \emph{enthalpy}.
  Ah well, such is the arbitrariness of a symbol's meaning. 
  (I suppose it's fitting.)
}
Usually, the \(log\)s written above are base-2. This permits a way of
thinking of the value of the Shannon entropy: if I'm only allowed to ask
the same series of questions to you to figure out which symbol you want
to send me and I know the actual probability distribution \(p\) of
symbols, how many questions should I expect to ask (i.e. mean/average)
before I figure out the answer? The cross-entropy is the same thing,
expect I don't necessarily know the actual probability distribution
\(p\) of symbols, I just think I do (and I think it's \(q\)) and base my
series of questions based on \(q\).

\section{KL divergence and mutual information.}\label{kl-divergence-and-mutual-information}


Now, hopefully that makes it clear that \( p \neq q \implies H(p,q)
> H(p) \) --- if I don't know the actual distribution, I'm
not going to be able to answer the most optimal series of questions.
This suggestions to us a notion of ``distance'' (i.e. a metric) between
the probability distributions \(p\) and \(q\):
\[ KL(p \mid\mid q) = H(p,q) - H(p) \]

This metric is called the \textbf{Kullback--Leibler divergence}
\index{KL divergence} \index{relative entropy (information theory)}
(because
names) or the \textbf{KL divergence} (because initialisms), or seemingly
most rarely but probably most clearly the \textbf{relative entropy}. Out
loud, you'd say \(KL(p \mid\mid q)\) is ``the {[}blah{]} of p with
respect to q". The closer the KL divergence is to 0, the closer \(q\) is
to being \(p\), and \( KL(p \mid\mid q) = 0 \implies p = q \) at every
point in the domain of \(q\) (which is the same as the domain of \(p\)). 
(From the above formula, 
it should be clear that the KL divergence is not symmetric. The
first argument is the ``correct'' distribution and we're measuring how suboptimal
the second argument/distribution is at replicating the first one.)

This makes describing the
\textbf{mutual information between two random variables \(X\) and \(Y\)}
\index{mutual information (information theory)}
in terms of a KL divergence fairly intuitive. Just
running off the name, if X and Y were independent, you'd expect no
mutual information between them --- 
observing one variable wouldn't tell you anything about the other variable, you don't gain
a lot of information about one from the other. 
In such a case, we know something about their probability distributions,
namely that they're independent, i.e., \(P(X,Y) = P(X)P(Y)\), which implies 
\( KL\left(P(X,Y)\mid\mid P(X)P(Y)\right) = 0\). 
And in fact, one way of writing the mutual information 
between random variables X and Y is exactly

\[ I(X;Y) = KL\left(P(X,Y)\mid\mid P(X)P(Y)\right) \]
\\
which I think is pretty neat.
\\
\\
- DK (4/24/18)

\section{Entropy of variables vs. entropy of distributions}\label{entropy-of-variables-vs-entropy-of-distributions}

Now, there is another way of approaching mutual information by 
defining a conditional entropy between two random variables 
(itself requiring a defintion of joint entropy in order to put
the equation in a ``plug n' chug'' form). This raises some questions, the
first and most pressing of which being ``\emph{Variable?} We've been talking
about distributions this whole time!''\footnote
{
  More an outcry than anything, but still appropriate.
} 
, and the answer to which is ``Yes, variable.'' Since we've been 
focusing on distributions this whole time, the switch to a random variable \(X\) is
actually fairly straightforward -- we use the distribution from which \(X\) is drawn.

More explicitly, say \(X\) is drawn from a probability distribution \(p\). Then the
\textbf{entropy of \(X\)} is
\index{entropy (information theory)!for random variables}

\[ H(X) = \sum_{x \in X} p(x) \log\left(\frac{1}{p(x)}\right) \]

This may be reminiscent of our \(\tau_{a \sim q}\) from earlier, except that now we're using
the actual distribution of \(X\) (which is \(p\)), so it'd be \(\tau_{x \sim p}\). 

We can sort of think of it like this: the information entropy only really makes sense
for probability distributions. So, if we want to figure out how hard it 
is to encode a random variable \(X\),
we kind of ``pull out'' the probability distribution that \(X\) is drawn from and
use that in our formula.
\\
Now, unfortunately, the notation gets muddy when we allow ourselves to shove in
these random variables as arguments. For example, what does \(H(X,Y)\) (X and Y being
random variables) mean? H is provided two arguments -- is it the cross-entropy between
the underlying distributions of X and Y? Nope, it's the \textbf{joint entropy} of X and Y,
i.e., \index{joint entropy}

\[ H(X,Y) = H(p)\]

where \(p\) is the joint distribution of the random variables \(X\) and \(Y\).
Our main defense against confusing the two formulae is that random variables are
(normally -- hopefully!) denoted by capital letters while distributions are usually
denoted by lowercase letters.
\\
\\
You may wonder ``what's the point?'' Well, this begins to allow information theory analogues
for intuitions on random variables gleaned from statistics. The main missing piece at this point
is the \textbf{conditional entropy} \index{entropy (information theory)!conditional}
of a random variable Y with respect to X.
We'd expect that we could relate the joint entropy and conditional entropy of random variables
with one another, like how we can do so with the joint and conditional probability
distributions of the variables, especially since we've defined the entropy of a random
variable in terms of its probability distribution. And since:
\begin{enumerate}
  \tightlist
  \item 
    we applied a logarithm to the probability distribution (and took the expected value) when defining the entropy of a random variable; and
  \item
    the relationship between conditional and joint probability depends on multiplication:
    \[P(Y|X) \times P(X) = P(X,Y) \]
\end{enumerate}
we'd want the relationship between conditional and joint entropy to hold via addition:
\[ H(Y|X) + H(X) = H(X,Y) \]

In fact, that's exactly the case! You can derive the formula 
for conditional entropy based on the above relationship.
And just to circle back to information gain, we'd expect that we might gain some information
about a random variable X when we observe the random variable Y, depending on how the joint
distribution P(X,Y) compares with P(X). Earlier, we defined the mutual information in terms
of a KL divergence:
\[ I(X;Y) = KL\left(P(X,Y)\mid\mid P(X)P(Y)\right) \]

but we can also capture the quantity of ``how much does knowing Y (on average) help us figure
out X (and vice-versa)?'' using the language of entropies of random variables:
\[ I(X;Y) = H(X) - H(X|Y) \]

This is the expected amount of information gained 
by knowing the state of the random variable Y. 
Specific values of Y may give more information
about X -- may lead to a much tighter conditional distribution -- than others.
Then, you can look at it for specific cases, i.e., the
\textbf{information gain} \index{information gain}
about X from observing a specific state y for Y,
IG(X,Y = y), which is related to the mutual information via
\(E_{y \sim Y}\left[IG(X,Y)\right] = I(X;Y)\)).
So if we observe that Y took on the
value y, the information gain for X would be
\[ IG(X, Y = y) = KL\left(P(X,Y)\mid\mid P(X | Y = y)\right) \]

Writing the above in terms of information entropy would be
\[ IG(X, Y = y) = H(X) - H(X|Y = y) \]


where \(H(X|Y = y)\) is an expectation over the conditional distribution 
\(P(X | Y = y)\):
\[
  H(X| Y = y) = \sum_{x \in X} p(x|y) \times \log\left(\frac{1}{p(x|y)}\right)
\]

The main takeaway is that \emph{defining the entropy of a random variable as it is above
allows for the migrations of intuitions about random variables
from statistics and probability.} A worthy cause!\footnote{The notational confusion is still unfortunate though.}
\\
\\
- DK, 5/14/18

\newpage
\chapter{Statistics}\label{statistics}
\section{Bayes' Theorem}\label{bayes-theorem}

I can never seem to remember Bayes' Theorem directly, as they write it
out in textbooks. It makes so much more sense to me to think about it
from the relationships between conditional and joint
probabilites/distributions, and one of the common tricks to make Bayes'
Theorem useful in practice also comes to me far more easily when
explicitly thinking about events as being sampled from a \emph{sample
space} of possibilities/outcomes.

Consider two possible events A and B. Let's keep in mind that A is just
one possible outcome out of a set of possibilities, as is B; we'll say
\(\alpha\) is the set of possibilities from which \(A\) was drawn and
\(\beta\) is the set of possibilities from which \(B\) was drawn, i.e.
\(A \sim \alpha\) and \(B \sim \beta\) (this will be good to remember
later). Now:

\[ \text{Pr[A and B both occur]} := P(A,B) \]

Assuming individual events happen separately, there are two ways for
both A and B to occur: 
\begin{enumerate}
  \tightlist
    \item
      A happens first, then B happens. 
    \item 
      B happens first, then A happens.
\end{enumerate}
(``Duh'', I know.)
\\
Keeping in mind that the first event might affect the probability of the
second event occurring (i.e. remembering that conditional probabilities
exist), we can write:

\[ P(A,B) = P(A) \times P(B \mid A) = P(B) \times P(A \mid B) \]

And then it's simple to write out Bayes' Theorem as it's often written
(we'll write it perhaps a bit more evocatively):

\[ P(B) \times P(A \mid B) = P(A) \times P(B \mid A) \]

\[\begin{split} P(A \mid B) &= \frac{P(A) \times P(B \mid A)} {P(B)} \\
                        &= P(A) \times \frac {P(B \mid A)} {P(B)} \end{split}\]

Using the Bayesian interpretation: At first we thought the probability
that \(A\) occurs is \(P(A)\). After we saw that \(B\) happened, we
re-evaluate the probability that \(A\) occurs with a scaling factor \(
\frac {P(B \mid A)} {P(B)} \), which answers the following question: considering I've
seen \(B\) occur, how much \emph{more} likely did I observe \(B\) due to
\(A\) also being the case (the numerator \(P(B \mid A)\)), versus my having observed
\(B\) just because of how common/rare it is (the denominator, \(P(B)\))? To see
why this scaling factor makes sense, let's consider some edge cases:

\begin{itemize}
  \tightlist
  \item 
    \emph{A and B are uncorrelated}: Then observing B is irrelevant when it comes
    to predicting A. So our scaling factor should be 1. And in fact, the lack of 
    correlation implies \(P(B \mid A) = P(B) \implies \frac {P(B \mid A)} {P(B)} = 1\).
  \item
    \emph{A precludes B}: Then via contrapositivity, if we saw B, A must not be the case.
    So our scaling factor should be 0. And since A precludes B,
    \(P(B \mid A) = 0 \implies \frac {P(B \mid A)} {P(B)} = 0\).
  \item
    \emph{B implies A (and no other outcome from \(\alpha\))}:
    Then the total probability
    must become 1, and the scaling factor must come out to be \(\frac {1} {P(A)}\).
    We'll come back to this in a bit.
\end{itemize}

Also worth knowing the fancy terminology: 
\begin{enumerate}
  \tightlist
  \item
    the \textbf{\textit{a priori} probability} or just the \textbf{prior} 
    \index{prior (Bayesian statistics)}
    is what 
    we thought would be the
    probability that a random variable takes on a certain value before we
    observed anything. So the \emph{a priori} probability (or just prior)
    for the event \(A\) would be \(P(A)\). If we consider \(A\) to be a
    random variable instead of an event, we're guessing the distribution of
    \(A\) and so \(P(A)\) would be an \textbf{\textit{a priori} distribution}
    (or again, just the prior). 
  \item
    the \textbf{\textit{a posteriori}
    probability} or just the \textbf{posterior}
    \index{posterior (Bayesian statistics)}
    is what we think the
    probability that a random variable takes on a certain value is after
    observing something. In this case, the \emph{a posteriori} probability
    (or just posterior) of the event \(A\) after observing \(B\) is
    \(P(A \mid B)\). If we consider \(A\) to be a random variable instead of
    an event, we're guessing the distribution of \(A\) after observing a
    random variable/event B and so \(P(A \mid B)\) would be an
    \textbf{\textit{a posteriori} distribution}, (or again, just the
    posterior).
\end{enumerate}

\subsection{Substitution to the rescue.}\label{subsitution-to-the-rescue}
Now, in cases of inference via supervised learning, 
we have some data on the probability of one of the observable variables
\textemdash{} let's say we observe variables \(A \sim \alpha\) \textemdash{}
and the labels/targets for the observed variables (let's say \(B ~ \beta\)).
Then we can approximate \(P(B \mid A\)) and \(P(A)\) via empirical counts \textemdash{}
and would want to fit a continuous function, e.g. a Gaussian, to \(P(A)\) to
handle out-of-sample feature combinations.
So we've already guessed some prior \(P(A)\), and we're trying to improve it by
calculating the posterior \( P(A \mid B) \). But what if we don't know
\( P(B) \)? Do we need to also guess a function for \( P(B) \)? Well, that would
involve another outside assumption which may or may not be true, and usually we want
to make as few assumptions as possible and
``let the data speak for itself''.
So then is our guessing and data collection all for naught!?
Thankfully, not so! The answer lies right under our noses -\/- or in
this case, in our previous calculations.

Consider \(P(A,B)\) again. What would we get if we added \(P(A,B)\) over
all possible values of A? (Remember we said that \(A \sim \alpha\), so A
could have been some other event within the set \(\alpha\).) That's
basically just saying that we don't care what value \(A\) is, so we end
up with \(P(B)\)!\footnote{
  `!' used to denote excitement, not factorialization.
  }
And conveniently, we'd already have a way to estimate these values:

\[\begin{split} P(B) &= \sum_{A \in \alpha} P(A,B) \\
                     &= \sum_{A \in \alpha} P(A) \times P(B \mid A)          \end{split}\]

Our summand is the same as the values we've estimated either by guessing
(\(P(A)\)) or from our data (\(P(B \mid A)\))! With that, we can rewrite
our earlier equation as

\[ P(A \mid B) = 
  \frac{P(A) \times P(B \mid A)} {\sum_{A' \in \alpha} P(A') \times P(B \mid A')} \]

With that, we can crunch the numbers and perform Bayesian inference like
a champ or have a machine do it for us like a prudent delegator.
\\
\\
Returning to our discussion of our scaling factor, we needed to show that if
we're trying to predict \(P(A \mid B)\) via
\[P(A \mid B) = P(A) \times \frac {P(B \mid A)} {P(B)} \]
and B implies A (and nothing else from \(\alpha\), then the right-hand side should equal 1.

Expanding P(B), we get
\[ P(A \mid B) = 
  \frac{P(A) \times P(B \mid A)} {\sum_{A' \in \alpha} P(A') \times P(B \mid A')} \]

Via our implication, we have that \(P(B \mid A') = 0 \forall A' \neq A\).
So the denominator simplifies to just \(P(A) \times P(B \mid A)\), which is exactly the
numerator! So then \(\left(B \implies A\right) \implies P(A \mid B) = 1\) (which implies
\( \frac {P(B \mid A)} {P(B)} = \frac {1} {P(A)} \)), as expected.
% Though
% for our everyday activities, we often don't have the luxury of having
% someone/something checking our heuristics. So it's always worth trying
% to keep in mind that oftentimes, many different factors that you may not
% know or take into consideration can culminate in observations that
% surprise you -\/- there's a good reason you aren't told to constantly
% get yourself tested for a medical condition if you don't believe to be
% at risk!
\\
\\
- DK (4/24/18)

\newpage

\chapter{Constrained Optimization
Problems}\label{constrained-optimization-problems}

Motivated to do this when I was reading
\href{https://arxiv.org/pdf/1606.05579.pdf}{this paper} and realized I
forgot how we get to/use the KKT conditions (which is implied in Eq. 2
in the paper).

\section{Equality constraints only: The Method of Lagrange
Multipliers/The
Langrangian}\label{equality-constraints-only-the-method-of-lagrange-multipliersthe-langrangian}

NB:
\href{https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/lagrange-multipliers-and-constrained-optimization/v/constrained-optimization-introduction}
{Khan
Academy's videos on the subject} really make clear one potential
intuition for one constraint.

\subsection{Intuition with just one
constraint}\label{intuition-with-just-one-constraint}

Say you want to optimize (maximize or minimize) a smooth function
\(f(\vec{x})\) subject to the constraint \(g(\vec{x}) = c\) (let's say
\(\vec{x} \in R^n\) -\/- and we may just write \(x\) instead of
\(\vec{x}\)). (It's worth remembering that the constraints themselves
can also be expressed as functions -\/- they just happen to be set to
specific constants.) So our goal is to find the best point(s),
\({\vec{x}^*}\). For the purposes of explanation, we'll say our goal is
maximization, but it all applies for minimization too.

A way to build up the intuition is to consider the contour lines of
\(f(\vec{x}) = m\) for particular values of m. Our goal then is to
maximize \(m\) while having \(\vec{x}\) satisfy \(g(\vec{x}) = c\). If
it didn't fulfill this second requirement, then we'd just be ignoring
the constraint and solving an unconstrained optimization problem -\/- in
which case, we'd just set the gradient equal to zero and solve (say,
what a useful thought -\/- let's put that in our back pocket for
later...).

Let's call the constraint contour line (which we aren't allowed to
change and is set to some constant \(c\)) \(G\) and the function contour
line (which we can change by varying \(m\)) \(F(m)\). If we think about
it for a bit, we'll see that \emph{solving our problem is analogous to
choosing the largest value of \(m\) so that \(F(m)\) ``touches'' \(G\) in
the fewest number of places while still actually ``touching'' \(G\).}
Consider the alternatives:

\begin{enumerate}
% \def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{\(F(m)\) does not touch \(G\) at all:} Well, that means that
  when we look at the set of points that comprise the contour line
  \(f(x,y) = m\) (i.e., the \textbf{level set} \index{level set}
   of \(f\) corresponding to
  a value of \(m\)), \emph{none} of those points lie on
  \(g(\vec{x}) = c\). So we ignored the constraint again -\/- oops.
\item
  \emph{\(F(m)\) touches \(G\) too many times:} This implies that
  \(F(m)\) cuts across \(G\) (if it didn't, then where did the ``extra''
  touches come from?) And in that case, why not increase \(m\) a bit
  more? We're assuming \(f\) is relatively well-behaved, so if you nudge
  \(m\) up a bit to \(m^+\), the contour line \(F(m^+)\) will be fairly
  close to \(F(m)\) -\/- and so, still cross \(G\) somewhere.
\end{enumerate}

(By the way, I've been using the tortured phrase "minimal number of
times but not zero like c'mon don't be cheeky" because there can be more
than one location on the constraint curve that have the same maximal
value for \(f\). Consider maximizing \(f(x,y) = x^2 y^2\) subject to
\(x^2 + y^2 = 1\). The symmetry leads to four points that satisfy the
criteria -\/- bump up \(f\)'s output higher and you're off the circle,
bump it down and you're cutting across the circle (and also not
maximizing \(f\)).)

Now, if \(F(m)\) and \(G\) just barely touch but do not cross, then
their instantaneous ``slopes'' at their touching point must be in the same
orientation and the contour lines are moving in the exact same
``direction'' -\/- if they weren't, then the touching point is a crossing
point. So how do we capture this notion?

\subsection{Doin' me some gradients}\label{doin-me-some-gradients}

The (nonzero) gradient of a function is always perpendicular to its
contour lines. (Seems like a deep statement, but with a bit of thought,
you can see that it just comes from the definition/intuition of contour
lines (on which the value is constant) and gradients (which points
toward the direction that increases the function output the largest -\/-
with no portion of the ``step'' being wasted on movement that would keep
the value constant).)

So, we can convert out ``contour lines \(F(m)\) and \(G\) are in the same
direction'' directly to ``the gradients of \(f\) and \(g\) are in the same
direction'', i.e. \[\nabla f = \lambda \nabla g \]

where \(\nabla\) is the usual gradient operator and \(\lambda \neq 0\),
the scalar proportionality constant (it's the \emph{direction} that
matters, not the magnitude), is called the \textbf{Lagrange multiplier}
\index{Lagrange multiplier}
(dude got a lot of things named after him).

It's actually worth looking at this a bit more. We originally framed our
goal by trying to get \(F(m)\) and \(G\) to touch as little as possible.
But we can also frame it in terms of \(\nabla f\) and its relation to
the constraint functions:

\[\nabla f = \lambda \nabla g \quad \impliedby \quad \nabla f \ \text{is perpendicular to the contour } G \]

It makes sense that the right-hand side would be the case -\/- if
\(\nabla f\) \emph{did} have some part of it along \(G\), then we're
wasting that part! Why wouldn't we go along \(G\) a bit more? We'd still
be meeting our constraint, and since we stepped partially along the
direction of steepest ascent, we'd have increased \(f\) while we were at
it! It'll be worth remembering this observation a little down the line,
so keep it in your pocket for later (or some other handy container, if
you are doomed to the fate of clothing without functional storage
capabilities).

Anyway, that's great and all, but we only have \(n\) equations (each of
the \(n\) elements of the vector formulation above) and now we have
\((n+1)\) unknowns (all the coordinates for \(x\), plus \(\lambda\)).
Well, there's a reason the above relation used a(n) \(\impliedby\). On
the left-hand side, we've only captured that the gradients have to be in
the same direction -\/- we haven't added our constraint! (The right-hand
side encapsulates both, since we have \(G\) as the contour corresponding
to the specific constraint \(g(x) = c\).) So our full set of \((n+1)\)
equations with \((n+1)\) unknowns is

\[\nabla f = \lambda \nabla g \] \[g(x) = c\]

Now go to town! Worth remembering that all of this provides
\emph{necessary} but \textbf{not} \emph{sufficient} conditions for
optimality. Sufficient conditions would involve, for example, proving
the that Hessian matrix of \(f\) is negative semidefinite when trying to
maximize \(f\) (analogous to the second-derivative test in the
single-dimensional case), and even if you manage that you're only
guaranteed local maximality. Sounds like a lot of qualifications, but
we've actually narrowed the search space a great deal with these
conditions, so it's not as terrible as it sounds.

\label{sec:the-lagrangian-a-packaged-function}
\subsection{The Lagrangian: a packaged function} 

The above system of equations works great for people, but people have
also spent so much time and energy to make computers solve math problems
for us! Most of these programs are particular good and finding the zeros
of a function (without any fancy constraints). So how could we repackage
the (n+1) equations above into one function we can find into a
zero-finder?

Well, let's rewrite the above equations first:

\[\nabla f - \lambda \nabla g = 0\] \[g(x) - c = 0\]

Alright, now what? Well, if we were to write something like

\[ L(x) = f(x) - g(x) \]

we'd be \emph{almost} there, because if we took the gradient of \(L\)
and set it equal to zero, we get the ``direction'' constraint back. But at
the moment, we're making it so that the magnitudes of the two gradients
have to be the same too (which doesn't have to be true) \emph{and} we
forgot to incorporate our constraint again!

Well, why don't we reintroduce \(\lambda\) as a variable in such a way
that it handles the proportionality problem \emph{and} have it so that
\(L_{\lambda} = g(x) - c\) (so that we reincorporate our constraint into
the function)? Might sound tricky, but in fact we can modify \(L\) to
satisfy these requirements fairly simply:

\[ \mathcal{L}(x, \lambda) = f(x) - \lambda \left(g(x) - c\right) \]

Now that it's achieved its final form (thankfully didn't take ten
episodes of powering up), we change \(L\) to \(\mathcal{L}\) and call it
the \textbf{Lagrangian} \index{Lagrangian}
of \(f(x)\) (because dude needs more things
named after him -\/- and you know, he \emph{did} revolutionize the study
of classical mechanics with this formulation).

Worth noting is that if we define the Lagrangian as

\[ \mathcal{L^+}(x, \lambda ^+) = f(x) + \lambda^+\left(g(x) - c\right) \]

we still get the same answer to our optimization problem -\/- the only
difference is that compared with the \(\lambda\) we get from the
\(\mathcal{L}\) formulation, \(\lambda ^+ = - \lambda\).

A neat consequence of the formulation of \(\mathcal{L}\) is that we can
consider \(\lambda\) as a measure of how much we could improve \(f(x)\)
by incrementing the value of \(c\) (which we've been considering a
constant) by a differential amount. While it may seem to be ``clear'' just
by taking \(\mathcal{L}_c = \lambda\), it's a bit more subtle than that,
since \(\mathcal{L}(x, \lambda; c)\) was formulated with \(c\) as a
constant. The proof for this observation involves:

\begin{itemize}
\tightlist
\item
  forming a new function,
  \(\mathcal{L}^*(x^*(c), \lambda ^*(c), c) = \mathcal{L}^*(c)\), a
  single-variable function that parameterizes the input coordinates of
  the answer(s) to the optimization problem (and also the Lagrange
  multiplier) with respect to \(c\);
\item
  doing the multivariable chain rule;
\item
  thinking a bit to notice that a lot of things equal zero to get that
  \(\frac{d\mathcal{L}^*}{dc} = \lambda \);
\item
  and, having remembered that we're interested specifically about the
  points on \(\mathcal{L}\) that optimize \(f\) (which is exactly what
  \(\mathcal{L}^*(c)\) captures), realizing that the above result
  implies that first statement of the paragraph before this bulleted
  list.
\end{itemize}

What a mouthful.

\subsection{Extension to more than one
constraint}\label{extension-to-more-than-one-constraint}

Would be kind of a shame if we did all of this just to solve problems
with just one constraint. But thankfully, the extension is fairly simple
to describe!

Say we want to optimize \(f\) subject to \(k\) constraints
\(g_1 = c_1, g_2 = c_2, \cdots, g_k = c_k\). Now, in all but the most
trivial of cases, it would be impossible to have the gradients of all of
these different functions in the same direction. Intuitively (-ish, and
assuming you feel comfortable-ish with concepts in linear algebra), if
they can't all be in the same direction, you'd think that the "next best
thing" would be that the gradient of \(f\) is in the same direction as
some linear combination of the gradients of \(g_1, g_2, \cdots, g_k\),
i.e. that

\[ \nabla f = \sum_{i=1}^k \lambda _i \nabla g_i \]

That statement above is in fact a condition that is met in the answer to
our optimization problem! (How convenient.)

Now we have \(n\) equations but \(n+k\) unknowns. We once again fix that
by actually incorporating our constraints:

\[ \nabla f = \sum_{i=1}^k \lambda _i \nabla g_i \] \[ g_1 = c_1 \]
\[ ... \] \[ g_k = c_k \]

We can once again package everything together as a Lagrangian by having
that \(\mathcal{L}_{\lambda _i} = g_i - c_i\):

\[\begin{split} \mathcal{L}(x, \lambda _1, \cdots, \lambda _k) &= 
                                            f - (\lambda _1 (g_1 - c_1) + \cdots + \lambda _k (g_k - c_k) ) \\
                     &= f - \sum_{i=1}^k \lambda _i (g_i - c_i) \end{split}\]

And Bob's your uncle.

\subsection{Explaining the convenience, and a more generalizable intuition.}\label{explaining-the-convenience}

Earlier we just kind of accepted the convenience of our guess, but it's
worth figuring out why it works. Remember that observation you kept in
your pocket (or other handy container)?

\[\nabla f = \lambda \nabla g \quad \impliedby \quad \nabla f \ \text{is perpendicular to the contour } G \]

If \(\nabla f\) had any part of it along \(G\), then we could step along
\(G\) and further increase \(f\). This sounds extensible to more than
one constraint! And in fact, the ``convenient'' result captures this for
the contour line created by the intersection of all the constraints:

\[
% \begin{multline*}  
  \nabla f = \sum_{i=1}^k \lambda _i \nabla g_i  \impliedby
  \nabla f \text{is perpendicular to the intersection
  of all constraint contours } \bigcap _i G_i 
% \end{multline*}
\]

Here, \(\bigcap _i G_i\) serves as the one contour line on which we can
move along that satisfies all the constraints. (Presumably such
continuous arcs exist -\/- otherwise, there are only discontinuities
(i.e. discrete points), and so each point in the set would have to be
checked individually.)

Now, since we're dealing with the same construct (a contour along which
we can move that satisfies the constraints), the same reasoning applies
pretty much verbatim -- \emph{\textbf{if \(\nabla f\) weren't perfectly
perpendicular to \(\bigcap _i G_i\), we'd be wasting the component of
the gradient going along \(\bigcap _i G_i\), \(\nabla f ^{\parallel}\).
So we'd just step along the contour and improve our result.}}\footnote{
  Excessive bolding, italicizing, and a gratuitous footnote 
  used to emphasize the fact that this is the
  mathematically rigorous ``intuition'' to have/remember.
}
So that explains why the right-hand side makes sense. But how does that
imply the left-hand side? Specifically, how do we get the linear
combination part, \( \sum_{i=1}^k \lambda _i \nabla g_i \)?

Well, since we're all on contour lines at the same time,
\(\bigcap _i G_i\) is necessarily perpendicular to all the gradients
\(\nabla g_i\). Then any linear combination \(\sum_{i=1}^k
\lambda _i \nabla g_i \) is perpendicular to \(\bigcap _i G_i\).
In fact, the gradients form a \emph{basis} for the space perpendicular
to \(\bigcap _i G_i\), because of the symmetric property of the
perpendicularity relation. More curtly, call the span of the gradients
\(S\). By construction, \(\bigcap _i G_i \perp S\), and by symmetry, \(S
\perp \bigcap _i G_i\). We want \(\nabla f\) to be perpendicular to
\(\bigcap _i G_i\) (as we've said before). Well then, that means
\(\nabla f \in S\), which implies that it \(\nabla f\) can be written as
a linear combination of \(S\)'s basis vectors, i.e., \( \nabla f =
\sum _{i=1}^k \lambda _i \nabla g_i \). Bam! Stick that beautiful
\(Q.E.D.\) square in the corner, we are done! \sout{Would do it myself were I
writing this in \(LaTeX\) and not Markdown.} 
Well, since we've migrated to LaTeX, I think we owe ourselves a box!
\begin{flushright}$\blacksquare$\end{flushright}
Totally worth it.
\\
\\
- DK, 4/28/18

\newpage

\section{Constraints with Inequalities: the Karush-Kuhn-Tucker (KKT)
Conditions}\label{constraints-with-inequalities-the-karush-kuhn-tucker-kkt-conditions}

\subsection{Not nearly as scary as they make it out to
be.}\label{not-nearly-as-scary-as-they-make-it-out-to-be.}

For all the pomp and circumstance around this, with the caravan of names
in the name itself and the esoteric terms used in its description like
``complementary slackness'' and ``dual feasibility'', the
Karush-Kuhn-Trucker (KKT) conditions aren't nearly as hard to follow as
one would expect if the method of Lagrange multipliers for multiple
constraints makes sense/is comfortable.

First, we pose the optimization problem in ``standard form'' (which mainly
just saves us from lugging around extra constants like we did with \(c\)
in the Lagrange multipliers explanation):

Optimize \(f(x)\) subject to \(g_i(x)\leq 0\), \(h_j(x)=0\), with
\(i \in \{1, \cdots, k\}\) and \(j \in \{1, \cdots, l\}\) (so \(k\)
inequality constraints and \(l\) equality constraints). Below, we'll
assume ``optimize'' = ``maximize''. We'll point out where changes will occur
if you're minimizing instead.

\subsubsection{Primal feasibility}\label{primal-feasibility}

This time, before we do anything else, we're going to stick down the
original constraints so we don't forget they exist:

\[ g_i(x)\leq 0 \ \forall \ i, \ h_j(x) = 0 \ \forall \ j \]

This is called the \textbf{primal feasibility} condition because it's a
condition for the feasibility of the original, main, ``primal'' problem.

\subsubsection{The dual formulation}\label{the-dual-formulation}

We refer to the original problem as the ``primal'' problem to contrast it
with the ``dual'' problem. That sounds all fancy, but we've made a dual
problem before when we formed the Lagrangian for our
equality-constraints-only version. That is, the 
\textbf{dual problem} \index{dual problem} is simply a
reframed but equivalent form of the primal problem. We did it before by
solving a function that had all our constraints wrapped in one clean
package (the Lagrangian form of the problem). And hey, that was both a
neat \emph{and} a useful idea, and those don't come around all that
often, so let's use it until it goes out of style.

Worth noting is that we want to make our dual problem mirror the primal
problem exactly (in \emph{optimal value} as well as optimal location),
i.e. form a strong duality, i.e. have no
\href{https://en.wikipedia.org/wiki/Duality_gap}{duality gap}. The
following provide \emph{necessary} conditions, but not \emph{sufficient}
conditions for a strong duality.

So what would be the dual problem? We can do the same thing we did for
the Lagrangian -\/- make a new function with some extra variables whose
partial derivatives yield the constraints of our problem. Let's try it:
\\
We define a function

\[L(x,\mu_1, \cdots, \mu_k, \lambda_1, \cdots, \lambda_l) = f(x) + \sum_i \mu_i g_i(x) + \sum_j \lambda_j h_j(x)\]

Maximize \(L\) (with no external constraints; i.e., find the locations
where \(\nabla L = \vec{0}\)). (We'll come back to minimization later.)
\\
\\
Alright, well that's certainly something. Now we can recover the
constraints by noting that \(L_{\mu_i} = g_i\) and
\(L_{\lambda_j} = h_j\). But there are some things that are still funky
with the inequality constraints here, so let's work on those.

(By the way, there are some signs we'd have to flip if we were 
doing a minimization instead. We'll discuss that later on.)

\subsubsection{Dual feasibility}\label{dual-feasibility}

For one thing, our dual problem won't mimic our primal problem of
optimizing \(f\) at all if we let any \(\mu_i\) be less than zero. If we
did, then we'd easily ``win'' the optimization game by choosing some \(x\)
such that \(g_i(x) < 0\) (which is still satisfies our primal
feasibility conditions), then setting the corresponding \(\mu_i\) to
arbitrariliy large negative numbers -\/- who cares about \(f(x)\)
anyway!? Oh wait, we do. So we should probably make sure that we can't
break our problem:

\[ \mu_i \geq 0 \]

This is called the \textbf{dual feasibility} condition because, well,
otherwise our dual problem isn't all that useful.

\subsubsection{Complementary slackness and
stationarity}\label{complementary-slackness-and-stationarity}

For one thing, we can notice that there are two possibilities for the
value \(g_i\) takes at an optimal point:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
% \tightlist
\item
  \(g_i(x^*) < 0\): That means the inequality constraint is not actually
  stopping the function from getting to a ``better'' location where
  \(g_i > 0\) (our optimal-point finder didn't hit a wall -- there's an
  open neighborhood around \(x^*\) that still satisfies the constraint),
  so the constraint isn't being restrictive at all. So we don't have to
  worry about it!
\item
  \(g_i(x^*) = 0\): The inequality constraint \emph{is} potentially
  stopping the function from reaching a more optimal point, so the
  constraint is actually affecting the outcome. So we should be sure to
  be othorgonal to the contour in our final answer.
\end{enumerate}

These observations inform the following two constraints,
\textbf{complementary slackness} and \textbf{stationarity}:

\[\begin{split} \mu_i g_i = 0 \ \forall \ i \qquad & \text{complementary slackness} \\
\nabla f = \sum _i \mu _i \nabla g_i + \sum _j \lambda _j \nabla h_j \qquad & \text{stationarity} \end{split}\]

Let's once again consider our two cases:

\begin{enumerate}
% \def\labelenumi{\arabic{enumi}.}
% \tightlist
\item
  \(g_i(x^*) < 0\): This constraint is inactive and so is not part of
  the contour lines we use to find the set of points on which \(f\) must
  lie. Recall that we form this set by evaluating the intersection of
  all the active constraints \(C\); that the span of the gradients of
  the active constraint functions form a space \(S\) orthogonal to
  \(C\); and that since \(\nabla f\) must be orthogonal to \(C\) in
  order to be a potential extremum, \(\nabla f \in S\) and can be
  written as a linear combination of the gradients of the active
  constraints. (Boy, another mouthful.) Well, \(g_i\) is not an active
  constraint, and so \(\nabla g_i\) is not part of the basis for \(S\).
  So we want its contribution in the stationary condition to be
  \(\vec{0}\). We can do this by setting the corresponding
  \(\mu_i := 0\). Not-so-coincidentally, this necessary consequence
  leads to compliance with the complementary slackness condition as
  well.
\item
  \(g_i(x^*) = 0\): The complementary slackness condition is met no
  matter what value \(\mu_i\) takes. And that's perfect: the constraint
  is active, so \(\nabla g_i\) is part of the basis for \(S\). So we
  \emph{need} \(\mu_i\) to be nonzero so that we can describe any vector
  in \(S\) (of which \(\nabla f\) is an element, as we talked about when
  we ``explained the convenience'' for the multiple-equality-constraint
  formulation earlier).
\end{enumerate}

So the two conditions are tied to one another in this interesting way
that makes it a more-or-less direct extension of the
multiple-equality-constraint formulation!

\subsubsection{Finishing our duel with
duals.}\label{finishing-our-duel-with-duals.}

...And with that, we've formed our dual problem with only equalities!
We'll copy them here to show we have enough equations for our unknowns:

\[\begin{split} h_j(x) = 0 \qquad            & \text{primal feasibility} \\
\mu_i g_i = 0 \ \forall \ i \qquad           &\text{complementary slackness} \\
\nabla f = \sum _i \mu _i \nabla g_i + \sum _j \lambda _j \nabla h_j \qquad & \text{stationarity}\end{split}\]

That's \(k + l + n\) equations for \(k + l + n\) unknowns! Now stick the
system of equations into a (probably numerical) zero-finder and you're
done!

\subsubsection{\texorpdfstring{On minimizing
\(f\)}{On minimizing f}}\label{on-minimizing-f}

Let's not forget that we had done all this assuming we were
\emph{maximizing} \(L\) (and thus \(f\)). If we were \emph{minimizing}
\(f\), we would be trying to minimize \(L\) and we could change the
above equations in one of the following ways:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Replace \(L\) with
  \(L(x,\mu_1, \cdots, \mu_k, \lambda_1, \cdots, \lambda_l) = f(x) - \sum_i \mu_i g_i(x) - \sum_j \lambda_j h_j(x)\)
  (note the minus signs). Replace the stationarity condition with
  \(-\nabla f = \sum _i \mu _i \nabla g_i + \sum _j \lambda _j \nabla h_j\)
  (again, note the minus sign).
\item
  Keep \(L\) the same. Replace the dual feasibility condition with \(
  \mu_i \leq 0 \)
\end{enumerate}

The sign changes just ensure that our dual problem is still well-formed
for the minimization problem (if you don't flip the sign somewhere, then
we can just make arbitrarily ``good'' values by playing with \(\mu_i\)
again; and if you flip the sign in the formulation of \(L\), you have to
make sure you update the gradient expression accordingly (Option 1)).

\subsection{If it's not that bad, why did you talk so
much?}\label{if-its-not-that-bad-why-did-you-talk-so-much}

...OK, so maybe this was a lot more to explain than I had given credit
at first. But it's really not that bad! The main intuition is that either:
\begin{enumerate}
  \tightlist
  \item
    the inequality constraint is lax and doesn't actually do any
``constraining''; or 
  \item 
    the inequality constraint is actually stopping us,
    in which case it just becomes another equality constraint!
\end{enumerate}
All the blah-blah-blah is to make sure we dotted our \(i\)'s and crossed
our \(t\)'s. (And boy, did we...)
\\
\\
- DK, 4/30/18

\newpage


\chapter{Machine learning methods.}\label{machine-learning-methods}



\section{Boosting.}\label{boosting}

There's a lot of buzz about the winning models of many Kaggle competitions 
use gradient-boosted trees. Considering its apparent effectiveness, it'd be worth understanding
what ``gradient-boosted trees'' are and how they work. But before we jump into \emph{that}, 
we should probably figure out what ``boosting'' means in this context.

\subsection{An analogy to Taylor expansions.}


\textbf{Boosting}\index{boosting (machine learning)} is a meta-algorithm involving
the ensembling of many ``weak'' learners to form arbitrarily ``strong'' learners.
A ``weak'' learner is a classifier/regressor (which we'll just call a \emph{predictor}) 
that can only be weakly correlated with the true underlying 
classication/regression (\emph{prediction}) model we are trying to 
learn. A ``strong'' learner should be able to approximate the true prediction model
arbitrarily closely.

To make sense of the somewhat vague denotation above, 
I like to think of writing a Taylor expansion of some (presumably non-polynomial)
function, say \(f(x) = e^x\). Let's say we're expanding about \(x_0 = c\).\footnote{
  Since the Taylor expansion is focused on being accurate
  \emph{in the neighborhood of the point the expansion is centered around}, 
  we can interpret this ``Taylor model'' as weighting inputs/examples near \(c\) 
  as far more important than examples from other parts of the input space.
} 
Then the n'th-order Taylor expansion is
\[ T_{n,c}(x) = \sum_{i=0}^{n} f^{(i)}(c) \times \frac{(x-c)^i}{i!}\]

In a sense, each term of the sum is weakly correlated with the target function $f$ in that
if we consider the \(i\)'th term \(T_n[i]\), its \(i\)'th derivative is equal to 
that of \(f\) at \(x = c\) and so can be called a ``weak predictor'' of \(f\):
\[T_{n,c}[i]^{{i}}(c) = f^{(i)}(c) \]

Another important note is that none of our weak predictors are redundant; each of them
contains at least \emph{some} new information about \(f\).\footnote{
  If you fancy appropriating a linear algebra term, you can also consider each of the weak
  predictors ``linearly independent'' of each other. 

  This is actually more on-the-nose than
  you might think. Just as we can think of vector spaces of \(R^n\) as being spanned by
  \(n\) linearly independent basis vectors, we can think of a 
  \emph{function space}\index{function space} being
  spanned by an infinite number of linearly independent basis vectors, some of which
  are \((x-c)^0, (x-c)^1, (x-c)^2, \cdots\). 
  So a Taylor expansion approximates a function \(f\) using a linear combination of
  only the basis vectors corresponding to polynomials, with each basis vectors scaled 
  by a certain amount (\(f^{(i)}(c) \times {i!}\)).
}
Alone, \(T_n[i]\) is a pretty underwhelming approximator \textendash{} it's only guaranteed to
approximate \(f\) in a specific way at one specific point. 
That isn't necessarily useful for our goals \textemdash{}
if we tried approximating \(e^x\) with the Taylor expansion's second term
\(T_{n,c}[i = 1](x) = (x - c)\), we'd be off by hundreds, thousands, millions almost everywhere!

The power of our ``weak predictors'' comes when we \emph{combine} them in some way 
\textemdash{} in the case of the Taylor expansion, a uniformly weighted sum. 
As we use more terms (i.e., more ``weak predictors''),
the ensemble becomes arbitrarily accurate to the target function \(f\) near \(c\) and so is
a ``strong predictor'' of \(f\).



\subsection{Meta-algorithm vs. algorithm}\label{meta-algorithm-vs-algorithm}

Importantly, we said that boosting is a \emph{meta}-algorithm, a meta-strategy that we
apply onto a strategy that \emph{actually} performs the approximating. 
In our ``Taylor boosting'' method, the strategy we use to approximate \(f\) is to create an
approximator \(T_{n,c}[i]\) that has the same \(i\)'th derivative of \(f\) at \(c\), and the
meta-strategy was to combine all those approximators together via addition. We can perform
boosting on other strategies. For example, consider a ``Dirac boosting'' method, where our
weak approximators are functions \(D_i\) where
\[D_i(x) = \delta(x-i) \times f(x) = \mathbbm{1}\left[x = i\right] \times f(i) \]

where \(\delta\) is the Dirac delta function and \(\mathbbm{1}\) is the indicator function.
So basically, our approximator memorizes the function at a single point perfectly and guesses 
that it equals zero everywhere else. Alone, this weak predictor is pretty terrible, but
we can perform ``Dirac boosting'' by combining many such Dirac approximators \(D_i\) in the
following way: given an input \(x\), we find the Dirac approximator whose center is closest
to \(x\), and output that as the result.\footnote{
  (Basically a \emph{k-nearest neighbors} regressor where \(k=1\).)
} 
The greater the number/density of Dirac approximators along the input space,
the more accurate our boosted approximator becomes!\footnote{
  Fun fact: Although there are cases where a Taylor expansion can perfectly recreate the
  target function (e.g., \(f(x) = e^x\)), our ``Dirac boosting'' method cannot \emph{ever}
  perfectly recreate \emph{almost any} function whose input space is over \(\mathbb{R}\), even if
  we use an infinite number of Dirac approximators in our ensemble! More specifically,
  it can never perfectly recreate any function \(f\) if
  \[\lambda\left(\{x \mid f(x) \neq 0\}\right) > 0\]
  where \(\lambda\) is the \emph{Lebesgue measure}\index{Lebesgue measure}.
  This is because 
  our Dirac ensemble can only be made up of a \emph{countable} number of approximators which
  each only memorize a single output, and it can be shown (via, e.g., Cantor's diagonlization
  argument) that the set of real numbers is a larger sort of infinity
  (whose size is 
  \emph{uncountably infinite}\index{sizes of infinity!uncountably infinite}) 
  than the set of natural numbers (whose size is
  \emph{countably infinite}\index{sizes of infinity!countably infinite}).
}

Even though we changed the algorithm by which we approximated our function (using our Dirac
approximators instead of Taylor approximators), we still employed the \emph{meta}-algorithm
of boosting to combine our weak predictors into a strong predictor. This should hopefully make
the distinction between the two clear.

\subsection{Where the analogy is left wanting.}\label{where-the-analogy-is-left-wanting}

Now of course, the analogy isn't perfect. A Taylor expansion is performed on a known target
function \(f\), but in a data science context, we don't have access to the actual
function that is responsible for our data. We \emph{do} have a dataset, a set of samples from
the actual function, which can serve as an \emph{approximation} of the true function.
So the best we can do is to use the data to create such an approximation (our model). 

Since the dataset is almost certain not to contain all of the intricacies of the underlying
function, we normally don't want to make our model fit the data \emph{too} well
(good ol' \emph{overfitting}\index{overfitting}). Instead, we tune our model so that it
minimizes a
\emph{loss function (or objective function)}\index{loss function}, 
which we hope we've set up cleverly enough so that
the model is a really good approximation of the underlying function when it reaches a minimum
of the loss function (for example, by introducing terms in the loss function that punish the
model for overfitting the dataset). We didn't specify an explicit loss function when
improving our ``Taylor'' and ``Dirac'' ensemble examples since we could explicitly observe the
actual function we were trying to fit.

% Keeping in mind all these
% \sout{caviar} caveats, we cam use the Taylor expansion as an example of 
% ``making a strong predictor by ensembling weak predictors''.

\subsection{Gradient boosting.}\label{gradient-boosting}



Alright then, so what is gradient boosting? 
% It's when we construct our \(i+1\)'th weak predictor
% based on the residuals of the target value (serving as a sort of predictor for the gradient
% of the loss function) and the value we predicted with our best-fit
% ensemble of the first \(i\) weak predictors, then weight the \(i+1\)'th weak predictor so that
% the ensemble of the first \(i+1\) weak predictors minimizes the loss as much as possible.

% That was a bit of a mouthful, so let's break it down a bit. 
To follow the usual algorithm
on which gradient boosting is employed, we'll use decision trees.
We have our model \(M(x)\), training examples \((x_a, y_a)\) and a loss function
\(L(y_a, y_p)\), where \(y_p = M(x_a)\) is our current best prediction for \(y_a\).\footnote{
  It's worth
  remembering that the loss function can be as fancy as we'd like as long as its gradient
  can be computed analytically from the actual and predicted values.
  % For example, if we had target vectors and we only cared about being in the same direction
  % (and magnitude didn't matter), the loss function could be the cosine of the angle between
  % the two vectors:
  % \(L(y_a, y_p) = \frac{y_a^Ty_p}{\lvert y_a \rvert \lvert y_p \rvert} + 1\)
}
We'll be creating various weak learners \(h_i(x)\), and we'll denote our ensembles of the first
\(i\) weak learners (including \(i=0\)) as \(E_i(x)\). We decide that the way we're going
to ensemble our weak learners is by a summation \(E_i(x) = \sum_{i} h_i(x)\),
and we enter the rodeo.
% for some \(\gamma_i\) for each learner that's learned during training 
% (as shall be explained below).

We start with a baseline prediction, the mean of \(y_a\), i.e. 
\(M(x) \leftarrow E_0(x) = h_0(x) = \bar{y_a}\).
Now, we're going to create and fit a new weak learner \(h_1(x)\). But since we're going to
make our stronger learner via \(E_1(x) = E_0(x) + h_1(x)\), we can just fit \(h_1(x)\)
to a sort of \emph{pseudo-residual determined by the loss function}, 
\(y_{pr,1} = - \frac{dL}{dE_0} \Bigr|_{\left(x_a, y_a\right)}\) 
for all \(\left(x_a, y_a\right)\) in our dataset. 
We then fit \(h_1(x)\) to the dataset of
pseudo-residuals \((x_a, y_{pr,1})\) as is appropriate for our weak learner \textemdash{}
in the case of decision trees, we perform recursive partitioning until some 
user-specified condition is met (e.g. any further partitioning would yield leaves with
fewer than \(c\) entries, or the \href{https://en.wikipedia.org/wiki/Information_gain_ratio}
{information gain ratio} of any such partition would be smaller
than some threshold) and then taking the mean of each leaf as the predictor for any
inputs that fall into that leaf at prediction time).
% \footnote{
%   If our weak learner were instead, say, \(h_i(x) = k_i \times x^i\), our method of fitting might
%   instead be to pick \(k_i\) such that the mean-square-error over the examples is minimized.
% }
% Now, we want our new ensemble \(E_1(x) = E_0(x) + \gamma_1 h_1(x)\) to minimize \(L\) as much
% as it can (or at least to improve the performance by at least a certain amount), so you find 
% \(\gamma_1\) that \(\frac{dL}{dy_p(\gamma_1)} = 0\). Since we've made our loss function
% easily differentiable by \(y_p\) and \(y_p\) has a simple relationship to 
% (For tree-based models, you can perform this for each individual leaf.)
% Now that you found \(\gamma_1\),
Now that we've fit \(h_1(x)\) to the pseudo-residuals as best as we can, adding \(h_1\) to our
previous best predictor will further decrease the loss function, so we 
update our model \(M(x) \leftarrow E_1(x) = E_0(x) + h_1(x)\). 

Now we're sort of at the same place we were at earlier, just with a slightly better model.
So we can create a new weak learner \(h_2(x)\) by fitting it to the ``second-order''
pseudo-residuals \(y_{pr,2} = - \frac{dL}{dM} \Bigr|_{\left(x_a, y_a\right)}\) 
(where now \(M = E_1(x)\)),
then find the constant multiplier that minimizes the
main loss function, then we update our model \textemdash and on and on we go.

Here, we can view the \((i+1)\)'th weak predictor as attempting 
to approximate the \emph{gradient}
of the loss function when the previous best predictor was used.
We fit to the \emph{negative} of the derivative since we're trying to \emph{minimize}
our loss function, so we want to correct toward the direction opposite the derivative
(and since we're ensembling via addition, the ``opposite'' part comes into play
at the pseudo-residual level).
Since the gradient of the loss function determines how we create our weak learners/ensemble, 
we call this method \textbf{gradient boosting}. And it apparently works wonders
when used with decision trees as the weak predictors.\footnote{
  For the right sorts of problems, when you tune it correctly!
  }

The expression for our pseudo-residuals may seem to come out of nowhere, but let's
consider \emph{actual} residuals for a moment: \(y_r = y_p - y_a\). When we fit
our weak learner to try and predict the negative of these residuals \(\{-y_r\}\) 
and add this to our ensemble, 
we're taking a step in minimizing the mean-square-error
of our predictor: \(L = MSE = \frac{1}{2} \left(y_p - y_a\right)^2\). And you can see that
\(- \frac{d(MSE)}{dy_p} = -y_r\). So rather than stick to just this one type of residual,
why not fit to the gradient of whatever loss function we'd like? 
Hence the term and our expression for ``pseudo-residuals''.

(Note: For further references and useful links, see this footnote.)\footnote{
(The explanation provided by Abhishek Ghose in \href{https://www.quora.com/What-is-an-intuitive-explanation-of-Gradient-Boosting}
{this Quora post} is quite good and helped me properly grasp
the concept of gradient boosting. Other main reference was
\href{http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/}
{this Kaggle blog post}. \href{http://arogozhnikov.github.io/2016/07/05/gradient_boosting_playground.html}
{This page} allows for an interactive demo of gradient-boosted decision trees in action.)
}
\\
\\
- DK, 5/17/18 (shoot, it's late again...)

\chapter{Physics.}\label{chapter:physics}



\section{The Boltzmann distribution}



I have an ultimate goal of having a good understanding as to why classical mechanics
fails to predict blackbody radiation properly and leads to the so-called ultraviolet
catastrophe. The situation involves analyzing the classical formulation of the
Rayleigh-Jeans law, which relies on the classical equipartition theorem, which for specific
cases can be derived from classical statistical mechanics.
\\
Eventually, we'd like to also take a look at \emph{why} particles absorb the frequencies
they do, which involves understanding molecular orbital theory, and so the Schrodinger equation,
and so the Hamiltonian, which implies Hamiltonian mechanics, which is apparently an improvement
on Lagrangian mechanics, which is an application of the Lagrangian (which 
we've talked about before at
\ref{sec:the-lagrangian-a-packaged-function}
and so would be our
jumping-off point).
\\
But first, counting. (And by the way, a lot of the insight I gained came from
\href{https://courses.physics.ucsd.edu/2017/Spring/physics4e/boltzmann.pdf}
{this source} (which isn't explicitly mentioned in this writeup itself).)

\subsection{Stating our problem.}

Let's say we have a system with \(N\) \emph{indistinguishable}
particles 
(i.e., the particles are all of the same ``type'') 
and a total energy
\(E\) that can be swapped freely between the particles at increments of \(\Delta E\) (and
\(E\) must be an integer multiple of \(\Delta E\)), say
\(E = k\Delta E\). We presume that our system is in 
thermal equilibrium and no energy or particles enter or exit the system (i.e. that the system
is \textbf{closed}, and more specifically a
\emph{\href{https://en.wikipedia.org/wiki/Microcanonical_ensemble}{microcanonical ensemble}}.).
Given this situation, how can we figure out the probability
that this system is in a particular state?


Before we continue, we need to determine exactly what we mean by ``state''. We consider
two different levels of a state's description:
\begin{enumerate}
  \tightlist
  \item
    \emph{macrostate}: This description only has a summarized view of the system. In this case,
      it describes how many particles at each energy (in terms of a multiple of \(\Delta E\)).
      \footnote{
      The macrostate focuses on energy because of energy's importance in physical systems.
      Minimizing some function of energy is often used to solve problems in Lagrangian and
      Hamiltonian mechanics, and more generally in
      \href{https://en.wikipedia.org/wiki/Calculus_of_variations}{the calculus of variations}
      (which I hope to better understand and write about at some point).
      }
  \item 
    \emph{microstate}: This description holds information not only about the macrostate,
      but also about other parameters that would predict how the macrostate would evolve
      (over time). In our case, in addition to the
      energies of each particle, 
      we'd need to know each particle's position and momentum to know which particles collide
      with which other particles on the next ``timestep'' of the system's (\emph{dynamic})
      equilibrium. In this case, since we know each particle's momentum, 
      \emph{we also know each particle's energy.}
\end{enumerate}

We're interested in determining the probability that a system is in some particular
\emph{macrostate}.

\subsection{A lazy way out.}
One way we could potentially figure out our macrostate distribution
is by starting at some given
microstate, and observe the system evolve over a really long time, recording the macrostates
at every differential step in time. The assumptions we'd be making were that:
\begin{enumerate}
  \tightlist
  \item
    the entire 
    microstate space is connected, i.e., 
    that there is a path from any one microstate configuration
    to any other microstate configuration (otherwise, we'd miss the macrostates
    associated with the disconnected microstates); and that
  \item
    the proportion of times a macrostate appears in our record of the system's evolution
    corresponds to the probability that the macrostate would occur. \footnote{
      This is actually an important assumption \textemdash{} that observing one system for
      a very long time tells us information about what a distribution of a large ensemble
      of systems would look like, and vice-versa.
      Such systems are called \href{https://en.wikipedia.org/wiki/Ergodicity}{ergodic systems}.

      It's also important to remember that this is an \emph{assumption}. There are cases
      where a system can be shown not to be ergodic.
    }
\end{enumerate}
The specifics of how the simulation ran would also imply other assumptions, which will
be discussed below. But we can approach it through
more direct means.

\subsection{Counting (is hard).}\label{subsec:counting-is-hard}

Let's try to count how many microstates correspond to each macrostate. Before we can
do that, we need to determine all the valid macrostates.
%  remembering that
% we're considering indistinguishable particles. 
We denote the number of particles with energy \(i\Delta E\) as \(n_i\) 
(in terms of integer multiples of \(\Delta E\)) that particle \(i\)
has as \(n_i\), then the valid macrostate configurations are
vectors \(\vec{n} = (n_0, n_1, n_2, \cdots, n_k)\) such that %its first moment equals N:

\[\sum_{i=0}^{k} i \times n_i = k, \quad \sum_{i=0}^{k} n_i = N\]

Figuring out which vectors \(\vec{n}\) for which this holds is a bit difficult to describe
cleanly. The problem can be described as a form of the
\href{https://www.geeksforgeeks.org/dynamic-programming-set-7-coin-change/}
{coin change problem}.\footnote{
  Which is itself a specific form of the
  \href{https://en.wikipedia.org/wiki/Knapsack_problem}{knapsack problem}.
}
\index{coin change problem}
In our case, the coins have value \(1, 2, \cdots, k\), and we remove sets of cardinality
greater than \(N\).\footnote{
  In this case, we're allowing sets to contain duplicates of the same element.
 }
The remaining sets are distinct valid macrostates. We can get each
set into the form of \(\vec{n}\) by setting \(n_i\) as the number of times \(i\) appears in
the set, and setting \(n_0 := N - \sum_{i=1}^{k} n_i\).
We'll refer to the set of all valid macrostate configurations as \(V\).


Now that we have valid macrostates, we can count how many \emph{different} 
microstates yield a given
macrostate. If all \(N\) particles were distinguishable, then every rearrangment of them
would result in a distinguishable microstate that would correspond to the same macrostate,
and so the number of microstates that make up a macrostate would be \(N!\) for any given
macrostate.

However, we're dealing with \emph{indistinguishable} particles, so we wouldn't notice if
we swapped around particles at the same energy state \(i\). We have to divide out these
redundant occurrences, because otherwise we're overcounting the \emph{number} of different
microstates.
There are \(n_i\) such particles at any given \(i\), and so the total number of different
microstates for a given macrostate \(\vec{n}\) would be
\[w\left(\vec{n}\right) = \frac{N!}{\prod_{i=0}^{k}n_i!}\]

If we make the reasonable assumption that all distinguishable microstates are equally likely
(after all, if none of them are more energetically favorable than any of the others, 
why would there be favorites?),
\(w(\vec{n})\) provides a way of \emph{weighting} the different macrostates 
\(\vec{n}\).
\footnote{
  If, for some reason, this assumption doesn't sit well, 
  one need only tweak \(w(\vec{n})\) to their liking.
  But considering experiments match the previously made assumption, Occam's
  Razor seems relevant.
}
% \footnote{
%   If, for some reason, you wanted to weight microstates differently, you'd have to
%   use \emph{generating functions}\index{generating function}, as described in
%   \ref{sec:generating-functions}. In that case, our generating function would index
%   by microstate (with the probability of that microstate being the index's coefficient)
% }
We can calculate the probability that the system is in a \emph{macrostate} corresponding
to \(\vec{n}\) via a proportion:
\[P(\vec{n}) = \frac{w(\vec{n})}{ \sum_{\vec{n}' \in V} w({\vec{n}'}) } \]

\subsection{Counting Is Hard II: Probability Strikes Back.}

\emph{6/6/2018 Update:} It has come to my attention that
\href{https://courses.physics.ucsd.edu/2017/Spring/physics4e/boltzmann.pdf}{my original main source}
could have been a bit clearer by stating that the assumption of a system with
total fixed total energy
(a \emph{microcanonical ensemble}, which eventually reaches thermal equilibrium within itself)
is appropriate for the discussion of the previous section,
whereas a more ``relaxed'' assumption of a system with fixed \emph{temperature}
\footnote{
  Remember that temperature (``intuitively'', based on kinetic theory) 
  is related to the average \emph{kinetic} energy
  of particles in a system. Therefore, a fixed temperature only implies a fixed total
  \emph{kinetic} energy within a system, not total energy (which could also include
  potential energies of various forms).
  \par
  It's worth noting that in thermodynamics,
  temperature is actually used as an
  \href{https://en.wikipedia.org/wiki/Temperature\#Temperature_as_an_intensive_variable}{intensive variable of the system}, 
  but it would take quite a while to justify/explicate that.
  The above kinetic-theory way of thinking about temperature
  should be good enough to explain where the ``extra'' energy
  could come from in a system with fixed temperature.
}
(a \emph{canonical ensemble}, which reaches thermal equilibrium with the system's surroundings,
an entity which can supply energy into the system)
is more appropriate in the following discussion wherein we
end up assuming independence of energies between particles inside the system 
\textemdash{} they can independently ``get'' their energy from the system's surroundings
and store it as potential energy, thereby ``gaining'' energy without violating the
fixed temperature stipulation.
(No wonder I needed to go through so much mental gymnastics to try and rationalize
the independence step...)

\par
Currently the below explanation remains unchanged since this discovery.
Once the section is adjusted, this note will no longer precede it.
\\
\\
So we've figured out the probability of the \emph{system} being in some \emph{macrostate}.
But what if we're interested in individual particles?
What would be the probability that, given some total
amount of energy in the system \(E_{{T}}=D\Delta E\),
a particle would be at energy level \(i\Delta E\)?

Well, we know the probability of being in any particular macrostate, and within each
macrostate, we know the fraction of particles at energy level \(i\Delta E\) 
(that being \(n_i)\),
so we do a weighted sum over the macrostate probabilities:
\[Pr[\text{particle in system has energy }i\Delta E] = P(i) =
\sum_{\vec{n}\in V}n_i \times P(\vec{n})\]

and we can have the expected number of particles in a particular energy level (assuming
\(N\) particles in the system):
\[\left<n_i\right> = \left<n\right>(i) = N \times P(i)\]

So what's the shape of \(P(i)\) (or \(\left<n\right>(i)\)) for large \(N\) and as 
\(\left(D,\Delta E\right) \rightarrow \left(\infty,0\right)\)?\footnote{
  The two limiting cases serve different purposes. We're trying to fill in our graph
  of (y = number of particles) vs (x = energy level). Large \(N\) makes the ordinate
  take on more continuous values (making it be better approximated by a continuous function),
  while \(\Delta E \rightarrow 0\) lets the abscissa take
  on more continuous values (which gives more points onto which one can fit a function).
} 
Well, let's consider how we expect the probability function to behave under a few actions.


First, let's figure out the probability that some particle is at energy \(i\) and
some other particle is at energy \(j\). The usual expression would be
\(Pr[i,j] = P(i) \times P(j \mid i)\), involving a conditional probability.
But wait, we're assuming that we have an arbitarily large
number of \(\Delta E\) that we're able to partition. If that's the case, then no matter
what \(i\) is, there's still (approximately) the \emph{same} number of \(\Delta E\) from which
the second particle can take its \(j\) \(\Delta E\)'s. That is, the probability distribution
(basically) \emph{doesn't change} after taking away \(i\) of the \(\Delta E\)'s, so
\(P(j \mid i) \approx P(j)\) and \(Pr[i,j] \approx P(i) \times P(j)\), implying that
the energy levels of two individual particles are (approximately) \emph{independent}. 
The approximation becomes more and more exact the more slices of energy there are, i.e.,
the closer \(\Delta E\) is to zero, 
and when \(\Delta E = dE\), independence holds exactly.\footnote
{
  This is because 
  there would always be an infinite number of \(\Delta E\)'s to choose from, no matter
  how many had been taken up via assignment to previous particles
  (since \(D \rightarrow \infty\))\textemdash{} a weird thought,
  but would technically be true.
  \par
  Worth noting that assuming that particle energy levels are 
  completely independent is actually a bit crazy-making.
  If any particle can be at any energy level in \(\left[0, E_T\right]\), then we can have, say,
  two particles with energy level \(2E_T/3\) and exceed the total energy of the system!
  Indeed, according to the model, we could change the total energy of the system by a factor of
  \(N\). Or indeed, have it be zero.
  \par
  And in fact, having the step size be infinitesimal would mean
  that \(idE = 0 \  \forall i \in \mathbb{N}\), so that every particle would have to have
  an \emph{uncountably infinite} multiple of \(dE\) in order to have \emph{any} energy.
  (Differentials are weird.
  As is approximating a discrete situation with a continuous probability function.)
  \par
  All this to say, we should remember this an \emph{approximation} and we should
  keep in mind how if we take the approximation to be true,
  we're bending our problem statement in quite an exotic way.
  Alas, this is the price we pay for clean derivation results.
}\label{foot:independence-weirdness}
% \footnote
% {
%   The assumption of independence can be made less crazy-making if we assume
%   only fixed temperature
%   (i.e., that the system is in thermal equilibrium with its surroundings)
%   and not fixed total energy.
%   In such a situation, individual particles could ``get'' their 
%   energy from the surroundings and ``store'' it as potential energy of some sort,
%   thereby allowing a method for particles to have energy states independent of each
%   other (assuming that the effect of other particles in the system
%   on a particle's potential energy is zero or negligible \textemdash{} which
%   is true for situations like a system of traveling light particles
%   or chemical particles with no (or mostly screened) charge in solution).
% }
So, in the limiting case, \(Pr[i,j] = P(i) \times P(j)\).


What about the probability that sum of two particle's energies equal \(i+j\)?
Once again, as argued above, in our limiting case, the energy of a particle is independent
of the erngy of any other particles. So the probability we're looking for is independent
of all the other particles in the system \textemdash{} it only depends on the two we're looking
at. Then we could describe the probability as
\[Pr[i+j] = 
  \sum_{k=0}^{\frac{i+j}{\Delta E}}\left(P(k)
  \times P\left(\frac{i+j}{\Delta E} - k\right)\right)
  = q(i+j) 
  \]

which enumerates over energy pairs
\( (0,\frac{i+j}{\Delta E}), (\Delta E,\frac{i+j}{\Delta E} - \Delta E), \cdots,
  (i, j), \cdots, (\frac{i+j}{\Delta E}, 0) \).


We've previously assumed that all microstates have equal probability of occurring.
Then that means \emph{all} of the energy pairs above (which are essentially microstates of a
two-particle ensemble) occur with equal probability.
Then
\[Pr[i+j] = q(i+j) = \left(\frac{i+j}{\Delta E} + 1\right) \times Pr[i,j] = B \times Pr[i,j]\]

But we saw earlier that \(Pr[i,j] = P(i) \times P(j)\), so

\[q(i+j) = B \times P(i)P(j)\]

That means that 
\(P\) must be some function such that \(P(i)P(j)\) becomes a function of \(i+j\) \emph{only} (because \(q(i+j)\) is, as we described above, a function of only \(i+j\)).
And the only type of function with such a property is an exponential function:\footnote{
  Note that the base could be any positive number that isn't 1 and the statement would still
  hold true. \(e\) is just the most convenient base to use.
}

\[Pr[\text{particle in system has energy }i\Delta E] 
              = P_{+}(i) = C_{+}\times e^{a_{+}i}
\]

where \(C_{+}\) and \(a_{+}\) have yet to be determined. We'll do that soon.
But you know,
I think it's a \emph{tad} unlikely that a particle is more likely to be in high
energy levels than low ones (especially considering our calculations have shown a decrease in
probability as energy level increased),
so let's first guess that the exponent is negative \textemdash{} if we happen to be wrong,
the as-yet-undetermined parameter in the exponent will end up being negative:

\[Pr[\text{particle in system has energy }x\Delta E] = P(x; \Delta E) = C\times e^{-ax} \]

And finally, let's make an alternate expression in terms of energy directly, as opposed to
in terms of discrete energy levels, as deriving the Boltzmann distribution will rely on
an upcoming assumption on the energy specifically:

\[Pr[\text{particle in system has energy }E] = p(E) = Ae^{-bE}\]

It's worth noting that this is actually a pretty big shift in frame. \(P(x)\) implies the energy
is discretized by a step size \(\Delta E\) and has arguments \(x \in \mathbb{N}\) no matter
how small \(\Delta E\) becomes. The equivalent point for \(P(x)\) in the function \(p\)
is at \(E = x\Delta E\), that is,

\[P(x; \Delta E) = C\times e^{-ax} \mapsto p(x\Delta E) = Ae^{-bx\Delta E} \]


That means unlike \(x\), as \(\Delta E \rightarrow 0\), the values
\(E\) can take on becomes more and more granular, 
to the point where \(E\) can become arbitrarily
close to any desired value.\footnote{
  Though technically, it can never be \emph{any} arbitrary value since the domain of \(E\)
  is always a subset of the rational numbers \(\mathbb{Q}\).
} Also, if an explicit upper bound of \(E_T\) isn't placed on the argument of \(p\)
(which is not done in the original derivation of the Boltzmann distribution 
and so will not be done here), then we're now allowing particles to
(very improbably, but technically) be able to take on arbitrarily large energies!

\subsection{A fork in the road.}

Now we can proceed with the derivation in two ways:
\begin{enumerate}
  \tightlist
  \item
    \emph{Double-down on the fact that we're analyzing cases where
     \(\Delta E \rightarrow 0\); that is to say,
    make \(\Delta E = dE\) and \(E\) continuous}:
    This would arguably be the more logical of the choices given our preceding argument.
    In order for our exponential function to fit the probability distribution \emph{exactly},
    we needed the energies of two different particles be independent of each other,
    and in order for \emph{that} to have been the case we'd need to take \(\Delta E\) to 0.
    % However, it's worth keeping in mind that now we've contradicting our argument in
    % a different way now: 
    % \emph{our probability distribution \(p(E)\) impliies that our particles can have
    % arbitrarily large amounts of energy.} Of course, the probability is preposterously low,
    % but it is nonzero.
  \item
    \emph{Continue our analysis under the assumption that \(\Delta E\) is not a differential}:
    This technically aligns with how we understand physics today \textemdash{} i.e., that
    energy is quantized by the frequency of radiation,
    \(\Delta E(\nu) = nh\nu\), with \(n \in \mathbb{R}\) 
    and \(h\) as \textbf{Planck's constant}\index{Planck's constant},
    \(h \approx \SI{6.626}{J.s}\).
    In this case, we'll have to accept that our argument,
    having previously assumed continuous \(E\), 
    doesn't take ``full advantage'' of that assumption.
\end{enumerate}

We'll do the continuous case first, as it leads us to the form of the actual Boltzmann function.
We'll do the discrete case afterward for a bit of fun, and because one of the big
mathematical tricks needed in the derivation will be useful later on.\footnote{ 
  Specifically, it will be useful when trying to ``patch'' the classical equipartition theorem 
  and Boltzmann function in order to describe \emph{blackbody radiation}
  by having the average energy of an emitted electromagnetic wave 
  depend on both temperature
  \emph{and} frequency \textemdash{} but we're getting way ahead of ourselves!
}

\subsection{Planning our attack.}

But what's our plan of attack in finding out the correct parameters for our probability
distribution(s)? It's the same plan that can be used
when finding the correct parameters for \emph{any} probability distribution 
\textemdash{} derive relationships
via the distribution's moments. We can say some random variable \(\epsilon\) takes on
values \(E\) for the continuous case and some random variable \(X\) takes on values \(x\)
in the discrete case. Then we can say that:
\begin{itemize}
  \tightlist
  \item
    the zeroth moment equals 1 (because they're probability distributions).
  \item
    The first moment (centered around 0) equals the mean, 
    \(E[X]\) (also written \(\left<x\right>\).
  \item
    The second moment (centered around \(E[X]\)) equals the variance, \(E[X-E[X]]^2\).
\end{itemize}

...And so on. In this case, we'll only need to go as far as the first moment.\footnote
{
  As it turns out, a different result (the equipartition theorem) will tell us the value
  of \(\left<x\right>\) for a system at thermal equilbrium at a given temperature \(T\),
  so we'll eventually be able to pull the distribution out of the theoretical and into
  the practical \textemdash{} but again, we get ahead of ourselves.
}


\subsection{The continuous case: An easy battle.}

So right now we want to figure out \(A\) and \(b\) in

\[Pr[\text{particle in system has energy }E] = p(E) = Ae^{-bE}\]

using moments. Well, since we're treating energy as continuous, our moments
are integrals (which are \emph{much} nicer to deal with than series).

First, the zeroth moment of the probability distribution should equal 1:
 
\[\int_{E=0}^{\infty} Ae^{-bE}dE = 1 \]
\[  \cdots \]
\[  A = b \] 

OK, cool, we've stripped away a degree of freedom, but we need to remove one more
before we have something usable. Note also that the infinite integral would not converge
for exponentials with positive exponents. So common sense prevailed!
\\
Anyway, onto the next moment. The first moment of the probability distribution
should equal the mean of the random variable of our distribution:

\[\frac{\int_{E=0}^{\infty} AEe^{-bE}dE} {\int_{E=0}^{\infty} Ae^{-bE}dE} = \left<x\right> \]
\[  \cdots \]
\[  b = \frac{1}{\left<E\right>} \]

Awesome! We have the parameters for our probability distribution, and we have our final result:

\[Pr[\text{particle in system has energy }E] 
        = p(E) = \frac{e^{-E/\left<E\right>}}{\left<E\right>}
\]

% So given that these microstates form the weighting for the macrostates,
% what's the probability distribution of these macrostates?
% Boltzmann used the calculus of variations (using the constraint that solutions must
% maximize entropy (since we're at equilbrium)) to get kBT...

If you use the result from the classical equipartition theorem\footnote{
  Which we'll hopefully go over later.
}
, you'd have that
\(\left<E\right> = k_BT\) and so 

\[p(E) = \frac{e^{-E/k_BT}}{k_BT}\]

And \emph{that} is the \textbf{Boltzmann distribution}\index{Boltzmann distribution}
in all its glory!\par

% Although it may seem like all this work to derive the Boltzmann distribution 
% was done for a fairly meager cause
% \textemdash{} just for a group of particles? \textemdash{}
% its importance is tremendous.

\subsection{The Boltzmann factor.}

A convenient thing about the Boltzmann distribution is how simple it is to calculate
the relative probabilities of a particle being in one state over another.
If we have two states with energies \(E_1\) and \(E_2\), we can calculate their
relative probability by taking their ratio (we'll keep the denominator
of the function as \(\left<E\right>\) for now):

\[
\begin{split}
  \frac{P(E_2)}{P(E_1)} &= \frac{\frac{e^{-E_2/\left<E\right>}}{\left<E\right>}}{\frac{e^{-E_1/\left<E\right>}}{\left<E\right>}} \\
    &= \frac{e^{-E_2/\left<E\right>}}{e^{-E_1/\left<E\right>}} \\
    &= e^{-(E_2-E_1)/\left<E\right>} \\
    &= e^{-\Delta E/\left<E\right>}, \ \Delta E = E_2 - E_1
\end{split}
\]

% If we substitute in \(\left<E\right> = k_BT\), we get

% \[ \frac{P(E_2)}{P(E_1)} =  e^{-\Delta E/{k_BT}}, \ \Delta E = E_2 - E_1\]

% Rearranging to solve for \(\Delta E\), we get:

% \[ \Delta E = -k_BT\ln(\frac{P(E_2)}{P(E_1)}), \ \Delta E = E_2 - E_1\]

% The above two forms are seen all around chemistry and biology, often with
% \(\frac{P(E_2)}{P(E_1)}\) being written as some single dimensionless quantity
% (for example, \(K_{eq}\) or \(Q\) in chemical equilibrium situations)



The \emph{Boltzmann factor}\index{Boltzmann factor} is simply the relative probability
of a certain state compared to the state corresponding to \(E = 0\)
(however that may be described):

\[
\begin{split}
  \frac{P(E)}{P(0)} &:= F(E) \\
  &= \frac{\frac{e^{-E/\left<E\right>}}{\left<E\right>}}{\frac{e^{-0/\left<E\right>}}{\left<E\right>}} \\
    &= e^{-E/\left<E\right>}
\end{split}
\]

You'll see that the relative probability reflects the real-life observation that
objects in the world are more likely to be found in low-energy states than high-energy
ones (and so ``prefer'' low-energy states to high-energy ones). Note that a ratio
of Boltzmann factors gives a ratio of relative probabilities of the two states:

\[\frac{F(E_2)}{F(E_1)} = \frac{\frac{P(E_2)}{P(0)}}{\frac{P(E_1)}{P(0)}} \\
            = \frac{P(E_2)}{P(E_1)}\]

% For whatever reason, in statistical mechanics they like calculating probabilities
% using Boltzmann factors explicitly rather than integrating over the probability distribution,
% so rather than:

% \[p(E) = \frac{e^{-E/k_BT}}{k_BT}\]

% in (classical) statistical mechanics you'll often see:

% \[p(E) = \frac{e^{-E/k_BT}}{\sum_i e^{-E_i/k_BT}}\]

% where \(i\) enumerates through all possible microstates and
% \(\sum_i e^{-E_i/k_BT}\) is given the grandiose-sounding name
% \emph{partition function}\index{partition function!classical form}.
% \sout{(And that's the very least of the grandiose-sounding names.)
% \par

% Although we calculated the Boltzmann distribution for an ensemble of particles,
% the choice of the word ``particle'' was largely just for convenience. We can make the
% same argument and use the Boltzmann distribution 
% for an ensemble of \emph{anything} if they follow the assumptions we made above.



\subsection*{The discrete case: A battle of wits.}

Alright, now the discrete case. For ease of notation, we will work with \(P(x)\) instead of
\(p(E)\) \textemdash{} again, the mapping between the two is \(E \mapsto x\Delta E\), so it
isn't too bad to interconvert:

\[Pr[\text{particle in system has energy }x\Delta E] = P(x; \Delta E) = C\times e^{-ax} \]

Again, we have the normalization condition:

\[\sum_{x=0}^{\infty} Ce^{-ax} = 1 \]

This is a bit worrying at first. But note that

\[\begin{split}
  \sum_{x=0}^{\infty} e^{-ax} &= \sum_{x=0}^{\infty} \left(e^{-a}\right)^x \\
                      &= \frac{1}{1 - e^{-a}} = (1 - e^{-a})^{-1} \text{ if } \left|e^{-a}\right| < 1
\end{split}
\]

from the formula for the sum of an infinite converging geometric series.
\\
With that, we have that

\[ C = 1 - e^{-a} \]

OK, let's move on to the first moment to figure out what \(a\) is in terms of \(\left<x\right>\):

\[
\frac{\sum_{x=0}^{\infty} Cxe^{-ax}} {\sum_{x=0}^{\infty} Ce^{-ax}} 
= \frac{\sum_{x=0}^{\infty} xe^{-ax}} {\sum_{x=0}^{\infty} e^{-ax}}
= \left<x\right>
\]

This looks absolutely terrifying at first. And for quite a while. \sout{And possibly always.}
But there's a clever trick.\footnote
{
  Which I take no credit for. I saw this trick in Chapter 1 of
  \href{http://www.sicyon.com/resources/library/pdf/eisberg_resnick-quantum_physics.pdf}
  {Eisberg and Resnick's \textit{Quantum Physics of Atoms, 2nd ed.}}
}
Remember that in general,
\[\frac{d}{dx}\left(\ln{f(x)}\right) = \frac{\frac{df}{dx}}{f(x)}\]

In the same vein, we evaluate

\[\begin{split}
  \frac{d}{da}\ln{\sum_{x=0}^{\infty} e^{-ax}} &=
  \frac{\frac{d}{da}\sum_{x=0}^{\infty} e^{-ax}} {\sum_{x=0}^{\infty} e^{-ax}} \\
  &= \frac{\sum_{x=0}^{\infty}\frac{d}{da} e^{-ax}} {\sum_{x=0}^{\infty} e^{-ax}} \\
      &= \frac{-\sum_{x=0}^{\infty}xe^{-ax}} {\sum_{x=0}^{\infty} e^{-ax}} \\
      &= -\left<x\right> \\
\end{split}
\]

So then
\[
  \left<x\right> = -\frac{d}{da}\ln{\sum_{x=0}^{\infty} e^{-ax}}
\]

but we already saw that
\(\sum_{x=0}^{\infty} e^{-ax} = (1 - e^{-a})^{-1} \text{ if } \left|e^{-a}\right| < 1 \), so 

\[\begin{split}
  \left<x\right> &= -\frac{d}{da}\ln{\left(1 - e^{-a}\right)^{-1}} \\
      &= -\left(1 - e^{-a}\right) \times \frac{d}{da}\left(1 - e^{-a}\right)^{-1} \\
      &= -\left(1 - e^{-a}\right) \times \left(-\left(1 - e^{-a}\right)^{-2}\right) 
      \times \left(e^{-a}\right) \\ 
  \left<x\right> &= -\frac{e^{-a}}{1 - e^{-a}} = \frac{1}{e^a - 1} \\
\end{split}
\]
% We'll see this form again later.
% More specifically, again remembering our mapping
% \(x\Delta E \mapsto E, a \mapsto b\Delta E, C \mapsto A\),
% we'll see the following expression again:

% \[\left<E\right> = \left<x\Delta E\right> \\
%       = \left<x\right>\times\Delta E = \frac{\Delta E}{e^{b\Delta E} - 1} \]

% and at that point, we'll be trying to derive \(\left<E\right>\) 
% given \(b\) (as it's derived for the Boltzmann distribution)
% and \(\Delta E\).
% \\
% Anyway, let's solve for \(a\):
Now let's solve for \(a\):

\[a = \ln{\left(1 + \frac{1}{\left<x\right>}\right)} \]

Interestingly, this changes the base of our exponent:

\[e^{a} = \left(1 + \frac{1}{\left<x\right>}\right) \]

and so our probability distribution becomes:

\[P(x;\Delta E) = \frac{1}{1 + \left<x\right>} 
      \times {\left(1 + \frac{1}{\left<x\right>}\right)}^{-x}
\]

The form is reminscent of solutions to various simple steady-state problems \textemdash{} 
but alas, it's much less aesthetically pleasing and much less
convenient to manipulate/work with than the Boltzmann distribution!
\sout{Yet another reason why quantum phenomena can be unpleasant to some.}

\subsection{Bolting beyond Boltzmann.}

All of these calculations have been made with the assumptions that:
\begin{enumerate}
  \tightlist
  \item
    Particles within the system can be distinguished from one another by the 
    energy state they are in (and particles within the same energy level are
    indistnguishable about each other); and
  \item
    Any number of our identical particles can occupy any energy state.
\end{enumerate}

These assumptions do not hold for particles in the regime of quantum mechanics.
Specifically, in an ensemble of \emph{bosons}\index{boson} (particles with integer spin),
within a particular macrostate,
individual bosons can \emph{not} be distnguished from each other, even by energy level.
That means there are \emph{no} distinguishable microstates \textemdash{} the most granular
information you can get is from what we've been calling the macrostate
(the vectors \(\vec{n}\) of dimension \(\frac{E_T}{\Delta E}\) we would calculate via
solving the equivalent coin-change problem as mentioned in
\ref{subsec:counting-is-hard}).
Moving forward from there would eventually
lead to the \textbf{Bose-Einstein distribution}\index{Bose-Einstein distribution}.\par

If instead of bosons we're dealing with \emph{fermions}\index{fermion} (particles with
\(\frac{1}{2}\)-integral spin), then the \emph{Pauli exclusion principle} also applies,
meaning that our second assumption would \emph{also} be incorrect and only two such
fermions could be in the same energy state.\footnote{
  We could get the valid macrostates
  by solving the coin-change problem where the are only two coins per denomination
  (or solving it without the constraint then filtering out those sets with more than
  two coins of the same denomination).
}
Moving forward from \emph{there} would
eventually lead to the \textbf{Fermi-Dirac distribution}\index{Fermi-Dirac distribution}.
\\
\\
...Phew, that was exhausting!
\\
\\
- DK, 5/29/18


\chapter{Drafts and WIPs}\label{drafts-and-wips}

(Avert your eyes! Unless you're fine with drafts and WIPs \textemdash{} in which case, enjoy!)


\section{The Fourier and Laplace Transforms.}



% good source (unfortunately discovered in the middle of the process of thinking up/writing up
% an explanation): https://www.youtube.com/watch?v=zvbdoSeGAgI

% Laplace trasnform *as phasors*!?
% https://www.youtube.com/watch?v=6MXMDrs6ZmA




General flow is:

\begin{enumerate}
  \item
    Explore the Taylor series in terms of linear algebra (countably infinite basis vectors).
    Consider where and how it lacks power (discrete vs continuous).
    From this discrete case, extend to a continuous case
    (\emph{un}countably infinite basis vectors!)
  \item
    Define \emph{linear independence} for vectors in function space, 
    and use this to determine what our basis vectors can be.
    Massage out pseudo-Laplace transforms.

\end{enumerate}

\subsection{Revisiting the Taylor series.}

Previously, we've discussed the Taylor series centered around a fixed point \(a\):
\begin{equation}
\begin{split}
  f(x) &\approx \sum_{n=0}^{k}\frac{f^{(n)}(a)}{n!}(x-a)^n  \\
       &= \sum_{n=0}^{k}F_D(n; a) (x-a)^n \text{ where } F_D(n;a) = \frac{f^{(n)}(a)}{n!} \\
       &= p(x;a,k)
\end{split}
\end{equation}\label{equation:taylor-series}

The rationale for the expression we choose for \(F_D(n;a)\) 
is that \(p(x;a,k)\) will have identical \(i\)'th order derivatives with \(f(x)\) at \(a\)
for \(k \in \{0, 1, \cdots, k\}\), i.e., that
\[p^{(i)}(x=a) = f^{(i)}(x=a) \ \forall \ i \in \{0, 1, \cdots, k \}    \]

So as \(k\) becomes larger and larger, \(p(x;a,k)\) acts more and more like \(f(x)\)
at \(x=a\),
which we hope makes it ``act'' more and more like \(f(x)\)
in a neighborhood around \(a\).\footnote
{
  This doesn't always work perfectly for all functions. For example,
  \(f(x) = 
  \begin{cases}
    0 & x = 0 \\
    e^{1/x^2} & x \neq 0
  \end{cases}
  \)
  has \(f^{(i)}(0) = 0 \ \forall i \in \mathbb{N}\), 
  so its Taylor series centered at \(x=0\) yields \(p(x) = 0\),
  which certainly doesn't capture how \(f\) acts outisde the origin!
}
Letting \(k \rightarrow \infty\) makes \(p(x;a,k)\) ``act'' \emph{exactly}
like \(f(x)\) at \(x=a\).
For some functions \(f\), 
we can show that \(p(x) = f(x) \ \forall x\in \mathbb{R}\) for \emph{all} 
inputs of the function
by taking the limit of \href{https://math.stackexchange.com/a/2136695}{Lagrange remainders}.
The most notable functions for this are \(f(x) \in \{e^x,\cos(x),\sin(x)\}\).\par

% But what if we looked at the Taylor series a different way, through the lens of linear algebra?
% To make notation a bit less cumbersome, let's have \(F_D(n) := F_D(n; a = 0)\) and
% \(
%   p_T(x) = \lim_{k\rightarrow \infty}P(x;a=0,k)
%         =\lim_{k\rightarrow \infty}\sum_{n=0}^{k}F_D(n) x^n
% \).\par

\subsection{Discrete function spaces.}

Now let's go on a seemingly random \sout{secant} tangent.\par
Consider some random polynomial \(p(x;k)\) of order no greater than \(k\).
We could describe it as weighted sum of monomials, where the coefficients
of the monomial \(x^n\) is given by the function \(A(n)\):

\[p(x;k) = \sum_{n=0}^{k}A(n) x^n\]
What does this remind us of?\par
Well, it kinda looks like a linear combination of vectors, doesn't it?
\\
\\
Consider a \(k\)-dimensional vector space with the basis
\(B = \{e_n \mid n \in \{0, 1, \cdots, k\}\}\).
If we wanted to describe some vector in this space, we could do so as a linear
combination of the basis vectors:
\[ \vec{v}_A = \sum_{n=0}^{k}A(n) e_n = [A_0, A_1, \cdots, A_k]\]

That alone is kind of boring. What becomes more interesting is if we can create an
\emph{isomorphism} between this
(to be honest, kinda bland) \(k\)-dimensional vector space and something
more interesting. If we think of a neat, valid isomorphism for our basis vectors,
some other neat consequences should follow. \par
Well, what if we make the mapping \(e_n \leftrightarrow x^n\) (for \(x \in \mathbb{R}\))?
This is a valid mapping, because we can describe the set of
\(\{x^n \mid n \in \{0, 1, \cdots, k\}\}\) as linearly independent, in that
there's no way to make a weighted sum of other monomials \(x^b, b\neq n\)
identical to \(x^n\) for all \(x\) in our domain of interest.\par
Let's keep this going. What would \(\vec{v}_A\) mean on the other side of our
isomorphism?
\[ \vec{v}_A = [A_0, A_1, \cdots, A_k] = \sum_{n=0}^{k}A(n) e_n
  \longleftrightarrow \sum_{n=0}^{k}A(n)x^n = p_A(x)  \]

where \(p_A(x)\) is the (no greater than \(k\)'th-order) whose coefficients
are enumerated by \(A(n)\). This means that if we agree on a valid ``encoding'' process
\textemdash{}
in this case, that we'll describe \(p_A(x)\) as a weighted sum of monomials \(x^n\)
\textemdash{}
then we can go from function to vector and vice-versa.
When we let \(k \rightarrow \infty\), we're allowing our vectors, described by \(A(n)\),
to describe the space of all possible polynomials \(P^\infty\).\par




Worth noting is that there's no reason why we \emph{have} to choose the function \(x^n\)
in our mapping
\(e_n \leftrightarrow x^n\)
. We can choose \emph{any} function where it makes sense to say
that the functions for different \(n\) are linearly independent of each other.
For example, we could choose to have the mapping
\[e_n \leftrightarrow e^{nx}\]
since the set 
\(\{e^nx \mid n \in \{0, 1, \cdots, k\}\}\)
is linearly independent.\footnote
{
  In fact, since \(e^nx = \left(\sum_{k=0}^{\infty}\frac{x^k}{k!}\right)^n\), we could view
  \(e^{nx}\) as a(n oddly) weighted combination of \(x^0, x^n, x^{2n}, \cdots\),
  implicitly describing an infinite-degree polynomial with potentially only a finite
  number of coefficients.
}
The more-or-less complete freedom in choosing our basis mapping will prove useful,
as some ``encodings'' (i.e. some sets of basis functions)
will be more useful in some contexts than others.
The main caveat is that subset of function space implied by your basis mapping
is always limited as to what functions it can
``encode'' fully and/or concisely 
\textemdash{} if you choose the mapping \(e_n \leftrightarrow e^{nx}\),
there is no \(A(n)\) such that \(\sum_{n=0}^{k}A(n)e^{nx} = x \), so we're definitely not
spanning \(P^\infty\) with this new mapping and if you're interested
in simple finite-degree polynomials, you're probably better off
with our earlier encoding.\footnote
{
  Worth noting is that trying to create a mapping that can recreate \emph{all}
  functions is not the best idea.
  Recall that if we designate an input \(x\) in some domain \(D_x\)
  and some output \(y\) in some domain \(D_y\),
  then a \textbf{function}\index{function} \(f \in \mathcal{P}(D_x \times D_y)\)
  is simply a set of ordered pairs \((x,y)\)
  where each \(x\) appears only once in the set.
  (Here \(\mathcal{P}\) denotes the \emph{powerset} operator
  and \(\times\) the \emph{Cartesian product} operator.)
  So there are \emph{many} wacky functions one could make and basically no
  hope in accounting for all of them in a simple-to-describe basis.
}

\subsection{Equipping our vector space.}

% Now here's something trickier: how do we map a \textbf{metric}, or a function describing
% the ``distance'' between two points, so that it captures how close a function is
% to a point outside our function subspace? After all, not all functions can be described as
% a sum of the natural-order monomials \(x^n, n \in \mathbb{R}\), so how do we choose
% the point that matches it the best? Well, that depends on what we're aiming for.
% If we only care about the polynomial \(p\) associated with our function
% having identical derivatives to the target function \(f\) at some point \(x=a\),
% we could have the metric associated with each point be 
% \(\sum_i|f^{(i)}(a) - p^{(i)}(a)|\)
Here's something trickier: how do we perform a projection on a function that's 
outside our subspace? That is to say, how would we actually \emph{calculate} the
function \(A(n)\)?\par
Well, so far we haven't touched on that. We've defined a vector space, but we
haven't defined a \emph{norm} \(|\cdot|\) or
an
\emph{inner product} \(\left<\cdot, \cdot\right>\) for our vector space meant to represent,
in this case, polynomials.
% http://mathworld.wolfram.com/InnerProduct.html
% http://mathworld.wolfram.com/SchwarzsInequality.html
which 
we'd want to have capture ``how much'' of one vector is along in another vector would
be used to calculate our coefficients \(A(n)\) by supplying \(e_n\) as one of the
inputs to our inner product.
Well, that depends on what our metric is for considering two
functions to be ``close''
% \begin{verbatim}
%   Need to figure out how metrics change.

%   ...?
% \end{verbatim}



\subsection{Re-revisiting the Taylor series.}

Let's consider the Taylor series again,
where we'll set \(a:=0\) and let \(k \rightarrow \infty\):

\begin{equation}
  \begin{split}
    f(x) &\approx \sum_{n=0}^{\infty}F_D(n) x^n \text{ where } F_D(n) = \frac{f^{(n)}(0)}{n!} \\
         &= p(x)
  \end{split}
\end{equation}

Relating the Taylor series to our vector-space isomorphism,
setting \(A(n) := F_D(n)\) picks a point \(\vec{p}\) in our vector space \(P^k\) that corresponds
to the polynomial \(p\) which is ``closest'' to \(f\)
in the sense that \(p\)'s value and first \(k\)'th derivatives agree with \(f\)'s at \(x=0\).
It's almost like \(p\) is \(f\)'s ``\emph{projection}'' onto \(P^k\) under our chosen
definition of ``closeness''.\par
Our point \(p\) \textemdash{} and as such, our function that captures the
coordinates of \(p\), \(A(n)\) \textemdash{}
definitely has to be related to our definition of ``closeness''.
In this case, the metric we're minimizing is
\(\sum_i|f^{(i)}(0) - p^{(i)}(0)|\).
If our definition of closeness was instead, say, that they have identical definite integrals
\( \int_{x=0}^{1}f(x)dx = \int_{x=0}^{1}p(x)dx \),
we'd have a different ``closest'' point in our vector space.
This also shows that the metric of ``closeness'' we use determines whether there is a
\emph{unique} \(A(n)\) for \(f(x)\) in a given function space. If we're considering the space
of polynomials, there is only one point that satisfies the ``equal derivatives at \(a\)''
constraint, but many that satisfy the ``equal definite integral over \([0,1]\)'' constraint.



% % Let's say if can find some isomo
% % where the basis vector
% % \(e_n\) corresponds to the coefficient of \(x^n\) for
% % \(n \in \{0, 1, \cdots, k\}\).
% % The set \(B = \{e_n \mid n \in \{0, 1, \cdots, k\}\}\)
% % is in fact a valid basis,
% % because there's no way to make a weighted sum of other monomials \(x^b, b\neq n\)
% % identical to \(x^n\) for all \(x\)
% % and therefore no way to create one basis vector from a linear combination of the others.
% % Then \(B\) forms a basis for the vector space in question.
% Let's call this vector space \(P^k\), for the space of all \textbf{p}olynomials with order
% no greater than \(k\) with domain \(\mathbb{R}\).\par
% If \(P^k\) contains all \(k\)'th order polynomials, how is
% \(p(x;k)\) represented?
% It's represented by the (zero-indexed) vector whose \(n\)'th index equals \(A(n)\) for all
% \(n \in \{0, 1, \cdots, k\}\), because in such a case, the vector is described by
% \[ \sum_{n=0}^{k}A(n) e_n = \vec{v} \]
% If we allow \(e_n\) to ``assume'' its identity as the monomial \(x^n\), then we get
% \[\sum_{n=0}^{k}A(n) e_n = \vec{v} \longrightarrow \sum_{n=0}^{k}A(n) x^n = p(x;k)\]

% \(A(n)\) encodes exactly the coefficients needed that get us to \(p(x;k)\).
% It just so happens that if we have some function \(f(x)\), the point in \(P^k\)
% that has identical derivatives to \(f(x)\) at \(x=0\) is the point reached by
% setting the coefficients via \(F_D(n) = \frac{f^{(n)}(0)}{n!}\).
% Letting \(k\) grow larger means that we're choosing a polynomial
% from a larger and larger space of polynomials
% (which is itself a subset of all possible functions\footnote
% {
%   Recall that if we designate an input \(x\) in some domain \(D_x\)
%   and some output \(y\) in some domain \(D_y\),
%   then a \textbf{function}\index{function} \(f \in \mathcal{P}(D_x \times D_y)\)
%   is simply a set of ordered pairs \((x,y)\)
%   where each \(x\) appears only once in the set.
%   (Here \(\mathcal{P}\) denotes the \emph{powerset} operator
%   and \(\times\) the \emph{Cartesian product} operator.)
%   So there are \emph{many} wacky functions one could make.
% })






\subsection{But first, let's talk eigenfunctions.}

In order to ``trust'' that the Fourier and Laplace transforms are capturing all of the
``content'' of our input function


\subsection{The Fourier transform and wiggles over time.}

The Fourier transform is the way one maps a function
\(f: t \mapsto y,\ t \in \mathbb{R}, \ y \in \mathbb{R}\)
to its uniquely equivalent function \(F: \omega \mapsto z, \ z \in \mathbb{R}^2\).
Looking at it more explicitly through the lens of linear algebra,
if we say \(f\) came from a space \(\mathcal{F}_t\), i.e., \(f \in \mathcal{F}_t\), and
likewise that \(F \in \mathcal{F}_\omega\), then the Fourier transform \(FT\) is a mapping
\(FT: \mathcal{F}_t \mapsto \mathcal{F}_\omega\).\par

The actual definition for the \textbf{Fourier transform}\index{Fourier transform} is
\begin{equation}
  F(\omega) = \frac{1}{2\pi} \int_{-\infty}^{\infty}e^{-i\omega t}f(t)dt
\end{equation}\label{equation:fourier-transform}


and can be derived from taking 
the limit of the \emph{discrete-time Fourier series} of \(f(t)\) as the function's
period \(T \rightarrow \infty\).\footnote{
  Which we hope to discuss further at some point.
}
The inverse map, aptly named the
\textbf{inverse Fourier transform}\index{inverse Fourier transform},
is 

\begin{equation}
  f(t) = {2\pi} \int_{-\infty}^{\infty}e^{-it\omega}F(\omega)d\omega
\end{equation}\label{equation:inverse-fourier-transform}

Here we use \(i = \sqrt{-1}\).
(We may flip-flop between \(i\) and \(j\) to denote \(\sqrt{-1}\).
It should be clear from context.)\par

OK, cool, but what does it mean? Well, first let's remember one of the neatest things
you can prove using Taylor series, i.e. \emph{Euler's formula}\index{Euler's formula}:

\[e^{ix} = \cos(x) + i\sin(x) \]

% Also, the set of sine and cosine functions (and in fact any two sinusoids that are not
% identical) are orthogonal, i.e.,
% Also, any two non-identical sinusoidal functions are orthogonal, i.e.,
Also, note that
% \(\sin(\omega t)\) and \(\cos(\omega t)\)
\(\sin(x)\) and \(\cos(x)\)
% TODO: define orthogonality more clearly 
% (integral is zero in product with anyone except itself)
are mutually orthogonal\footnote{
  That is to say,
  % \[\int_{-1/\omega}^{1/\omega} \cos(\omega t)\sin(\omega t)dt = 0 \]
  \(\int_T \cos(x)\sin(x)dx = 0 \)
  over their aggregate period \(T\).
  To be proven later. See in the meantime
  \href{http://tutorial.math.lamar.edu/Classes/DE/PeriodicOrthogonal.aspx}{this resource}.
},
and in fact all sinusoidal functions
\(\{\cos(ax) \mid a \in \mathbb{R}\}\) and \(\{\sin(bx) \mid b \in \mathbb{R}-\{0\}\}\)
are mutually orthogonal.\footnote{
  When \(b=0\), \(\sin(bx) = \sin(0) = 0 \forall x\) and so doesn't fit the definition 
  of orthogonality since \(\int_{-\infty}^{\infty}sin(0x)sin(0x)dx = \int_T 0 dx = 0 \ngtr 0\).
}
That means that if we were to describe functions in a vector space, then each 
sinusoidal function \(\cos(ax), a \geq 0\) and 
\(\sin(bx), b > 0\) 
would be linearly independent from one another.\footnote{
  Only one ``half'' of \(\mathbb{R}\) is needed to capture all non-redundant information
  as \(\cos(-x) = \cos(x)\) and \(\sin(-x) = -\sin(x)\).
}
\par

All this points to a \emph{different basis} on which we can describe functions.
The origina
% every point in F(\omega) has some information about f across *all* of t
% it's like having an infinite ``power series'' built
% using the orthogonal functions cos(wt) and sin(wt).
% We've already established that cos(wt) and sin(wt) are orthogonal,
% So the only parts of f(t) that contributes to F(w) is the part that would
% correspond to cos(wt) (or sin(wt)) in a ``frequency series'' of f.
% I.E., if we could say that f(t) = 1 + a_1cos(wt) + a_2cos(2wt) + ...
% then F(w) = \int cos(wt)f(t)dt = \int cos(wt) \times a_1cos(wt)dt



% It's important to remember that this is a transformation of \emph{function to a function
% with a different input space}, and not a transformation of our coordinate system.
% There is no clear way of s

% Considering that functions are themselves maps (from their input space to their output space),
% the Fourier transform is a map of maps.
% This may sound like voodoo magic, 
% That is to say, the Fourier transform is a mapping for



\subsection{The Laplace transform as a patch for the Fourier transform.}

What if the Fourier transform explodes?
For example, consider the function



% TODO: Try and get from
% Lagrangian -> Lagrangian mechanics -> Hamiltonian mechanics
% -> the Hamiltonian in quantum mechanics -> the Schrodinger equation
% -> molecular orbital theory -> Beer-Lambert Law and *absorption* of light in media
% next,
% what determines reflection vs transmittance (physics and light properties)
% finally,
% tie together to blackbody radiation and the ultraviolet catastrophe
% (interest in blackbody radiation came from reading it in the app Quantum)

% also before blackbody radiation, consider
% Boltzmann distribution / partition function
% approximation of multinomial distribution above with an exponential funciont
% via the *Central Limit Theorem* (explain)

% generator functions
% https://math.stackexchange.com/questions/1646101/distribution-of-the-sum-of-n-loaded-dice-rolls

% Boltzmann distribution (and statistics for for bosons and fermions) 
% -> one example of a derivation of the equipartition theorem
% -> classical Rayleigh-Jeans law -> UV catastrophe / blackbody radiation ()

% virial theorem

% generator functions to handle cases where states have unequal probabilities of
% occurring (a loaded die, for instance).

% ^ all of the above first wondered about ~5/26/18


% why is the reduced mass of a system calculated
% like the equivalent resistance of resistors in parallel? - 5/28/18

% What is up with the different ``forms'' of free energy? (Gibbs, Helmholtz, internal, enthalpy)

% what causes water bubbles at e.g. the base of a waterfall? (``Cavitation''?)

% harmonic mean vs geometric mean vs arithmetic mean: when is it appropriate to use which one?

\section{Generating functions: dealing with uneven probabilities.}\label{sec:generating-functions} % 5/27/18

% https://math.stackexchange.com/questions/1646101/distribution-of-the-sum-of-n-loaded-dice-rolls













\section{Central Limit Theorem} % 5/27/18




% \appendix

% \chapter{Weird things about integrals.}\label{apx:weird-things-about-integrals}

% In our derivation of the Boltzmann distribution, we reached the point where to derive
% the exponential form, we needed to assume independence of energies among the particles 
% in the system, an assumption which, 
% as mentioned in the footnote at \ref{foot:independence-weirdness},
% is a bit crazy-making (by exceeding the total energy of the system).
% It can be even weirder than that when we think about what we consider the differential
% \(dE\) to be.
% \\
% In standard analysis, differentials are not numbers, they are operators.

% And if we don't subscribe to the tenets of
% \emph{nonstandard analysis}\index{nonstandard analysis} wherein
% there exist hyperreals which are larger than the countable infinity \(\aleph_0\)
% and whose reciprocals are differentials (a potential extension of
% number theory that is realized
% if one takes \(\neg G\) from \emph{Godel's incompleteness theorem}
% to be an axiom), then things become even \emph{more} ridiculous.
% % [adapt the stuff below]
% % Or, if we stick to talking about the discrete energy steps, if we say there are an
% % infinite number of them, then no matter how large an energy step \(i\)
% % we assign to each particle, \(\sum_{i=1}^{N} E_idE = 0 \neq E_T \), 
% % so energy in the system completely disappears and all particles have
% % \emph{zero} energy! Unless we have an \emph{infinite} number of particles.
% % In which case, we would have that
% % \(\lim_{N\rightarrow \infty}\sum_{i=1}^{N} E_idE = E_T \), but still 
% % These are the questionable outcomes that com

    % Add a bibliography block to the postdoc
    
    



    % show index
      % add index to table of contents
      \clearpage % flush all floating figures, "officially" start on next page
      \addcontentsline{toc}{chapter}{Index}  % add entry to ToC
    \printindex % actually print the index
    
\end{document}
